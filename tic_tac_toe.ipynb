{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import random\n",
    "\n",
    "random.seed = 123\n",
    "MAX_GAMES_DEFAULT = 20_000\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "env = TictactoeEnv()\n",
    "Turns = np.array(['X','O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against\n",
    "Opt(eps_opt) for some eps_opt ∈ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm,\n",
    "run a Q-learning agent, with a fixed and arbitrary eps ∈ [0, 1), against Opt(0.5) for 20’000 games – switch\n",
    "the 1st player after every game.\n",
    "Question 1. Plot average reward for every 250 games during training – i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_string(bts, is_buffer = True):\n",
    "    _grid = np.reshape(np.frombuffer(bts),(3,3)) if is_buffer else bts\n",
    "    str_rep = ''\n",
    "    value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "    for i in range(3):\n",
    "        str_rep +='|'\n",
    "        for j in range(3):\n",
    "            str_rep += value2player[int(_grid[i,j])] + (' ' if j<2 else '')\n",
    "        str_rep+='|\\n'\n",
    "    str_rep+='\\n'\n",
    "    return str_rep\n",
    "\n",
    "def print_Q_val_with_moves(q_vals, descending=False, only_vals = None):\n",
    "    qv_temp = {grid_to_string(k): q_vals[k] for k in q_vals if any(map(lambda a: q_vals[k][a] != 0, q_vals[k]))}.items()\n",
    "    count_free_squares = lambda grid: sum([x=='-' for x in grid])\n",
    "    if only_vals is not None:\n",
    "        qv_temp = list(filter(lambda k: count_free_squares(k[0]) == only_vals,qv_temp))\n",
    "    for (i,j) in sorted(qv_temp, key=lambda k: count_free_squares(k[0]), reverse=descending):\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# desparate debugging\n",
    "_epsilons = []\n",
    "def get_empty_positions_from_grid(_grid):\n",
    "    '''return all empty positions'''\n",
    "    avail = []\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if _grid[pos] == 0:\n",
    "            avail.append(pos)\n",
    "    return avail\n",
    "\n",
    "class BasePlayer:\n",
    "    def get_empty_positions(self, _grid):\n",
    "        return get_empty_positions_from_grid(_grid)\n",
    "\n",
    "    def hash_grid(self, _grid: np.ndarray):\n",
    "        return _grid.tobytes()\n",
    "\n",
    "class QLearntPlayer(BasePlayer):\n",
    "    def __init__(self, _epsilon: float, _discount_rate_gamma = 0.99, _learning_rate_alpha = 0.05):\n",
    "        super()\n",
    "        self._epsilon = _epsilon\n",
    "        self._discount_rate_gamma = _discount_rate_gamma\n",
    "        self._learning_rate_alpha = _learning_rate_alpha\n",
    "        self._Q_values = {}\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        self._player = None # 'X' or 'O'\n",
    "\n",
    "    def prepare_new_game_(self, _player):\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        assert _player == 'X' or _player == 'O'\n",
    "        self._player = _player\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, _possible_moves, _grid_hash):\n",
    "        if len(_possible_moves) == 0:\n",
    "            q_val = self._Q_values[_grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        # Shuffle moves to prevent bias towards choosing the first thing in the list\n",
    "        # This is important especially at the beginning when all the q-vals are 0\n",
    "        # And we are therefore biased towards choosing the first avaliable move, e.g. (0,0) in the\n",
    "        # starting position\n",
    "        random.shuffle(_possible_moves)\n",
    "        return max(_possible_moves, key=self._Q_values[_grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, _grid_hash, _possible_moves):\n",
    "        if _grid_hash not in self._Q_values:\n",
    "            self._Q_values[_grid_hash] = {} if len(_possible_moves) > 0 else {'': 0}\n",
    "        for mv in _possible_moves:\n",
    "            if mv not in self._Q_values[_grid_hash]: self._Q_values[_grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, _grid):\n",
    "        grid_hash = self.hash_grid(_grid)\n",
    "        # Get moves\n",
    "        possible_moves = self.get_empty_positions(_grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        random_sample = random.random()\n",
    "        play_best_move = random_sample >= self._epsilon\n",
    "        if play_best_move:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self._curr_grid = _grid\n",
    "        self._curr_move = chosen_move\n",
    "        if DEBUG:\n",
    "            print('-----------------------------------')\n",
    "            print('Current position: ', '\\n' + grid_to_string(_grid, False))\n",
    "            print('Current Q-vals', self._Q_values[grid_hash])\n",
    "            print('Random sample ', random_sample, ' _epsilon ', self._epsilon, ' hence I chose ', \\\n",
    "                '*best*' if play_best_move else '*random*', ' move: ', chosen_move )\n",
    "            print('-----------------------------------')\n",
    "\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid, game_over, _reward):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_move, prev_grid = self._curr_move, self._curr_grid\n",
    "        self._curr_grid, self._curr_move = None, ''\n",
    "        new_grid_hash = self.hash_grid(new_grid)\n",
    "        prev_grid_hash = self.hash_grid(prev_grid)\n",
    "        # Get max_a (Q(S', a))\n",
    "        possible_moves_s_dash = [] if game_over else self.get_empty_positions(new_grid)\n",
    "        self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "        max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "        max_q_value = self._Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "        if DEBUG:\n",
    "            print('*** UPDATING Q VALS ****')\n",
    "            game_over and print('*Game is over*')\n",
    "            print('Prev_grid: ', '\\n' + grid_to_string(prev_grid, False))\n",
    "            print('Prev_move: ', prev_move)\n",
    "            print('new_grid: ', '\\n' + grid_to_string(new_grid, False))\n",
    "            print('max_val_action: ', max_val_action)\n",
    "            print('Q-vals before: ', self._Q_values[prev_grid_hash])\n",
    "\n",
    "        # Update according to Q-learning formula\n",
    "        prev_q_val = self._Q_values[prev_grid_hash][prev_move]\n",
    "        self._Q_values[prev_grid_hash][prev_move] += self._learning_rate_alpha*(_reward + self._discount_rate_gamma*max_q_value - prev_q_val)\n",
    "        if DEBUG:\n",
    "            print('_reward: ', _reward, 'max-q-val', max_q_value, 'discount', self._discount_rate_gamma,\\\n",
    "                'self._learning_rate_alpha',self._learning_rate_alpha)\n",
    "            print('Q-vals after: ', self._Q_values[prev_grid_hash])\n",
    "            print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_opt(_max_games_number, _our_player, _opponent_epsilon, _our_player_new_game_epsilon, _update_q_values, _progress_print=None, _throw_error_on_illegal_moves = True):\n",
    "    _rewards = [None for _ in range(_max_games_number)]\n",
    "    _turns = ['X','O']\n",
    "    opponent =  OptimalPlayer(epsilon=_opponent_epsilon, player=_turns[0])\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        env.reset()\n",
    "        _current_grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent.player = _turns[0]\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != _our_player._player\n",
    "        _our_player._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "        # epsilons[-1].append(_our_player._epsilon)\n",
    "\n",
    "        if (_progress_print and game % _progress_print == 0) or DEBUG:\n",
    "            print('Game ', game, ' begins.')\n",
    "            if DEBUG:\n",
    "                print('We play: ', _our_player._player)\n",
    "                input('awaiting input: ')\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.act(_current_grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(_current_grid)\n",
    "\n",
    "            # This if branch was added to make this work for DQN player. If this breaks for normal player\n",
    "            # it is likely because of it (JL, May 6 2022)\n",
    "            if (not _throw_error_on_illegal_moves) and (chosen_move not in get_empty_positions_from_grid(_current_grid)):\n",
    "                _current_grid = None\n",
    "                _end = True\n",
    "                _reward = -1 # You lose if you play an illegal move\n",
    "\n",
    "            else:\n",
    "                _current_grid, _end, winner = env.step(chosen_move, print_grid=False)\n",
    "                _reward = env.reward(_our_player._player)\n",
    "\n",
    "            ## This was also altered to make this work for DQN player  (JL, May 6 2022)\n",
    "            if _update_q_values and (opponent_turn and turn > 0 or _end):\n",
    "                _our_player.update_q_values_(_current_grid, game_over=_end, _reward=_reward)\n",
    "            if _end:\n",
    "                _rewards[game] = _reward\n",
    "                break\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q1_epsilon = 0.1 # Chosen because they use this in Q2 so this will allow us to nicely compare\n",
    "\n",
    "q1_max_games = MAX_GAMES_DEFAULT\n",
    "q1_q_learnt_player = QLearntPlayer(_epsilon=q1_epsilon)\n",
    "print_Q_val_with_moves(q1_q_learnt_player._Q_values)\n",
    "q1_avgs = []\n",
    "q1_rewards = []\n",
    "q1_total_wins = 0\n",
    "\n",
    "for game_epoch in range(q1_max_games//250):\n",
    "    if game_epoch % 10 == 0:\n",
    "        print('Game ', game_epoch*250, ' begins.')\n",
    "    run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q1_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q1_epsilon, _update_q_values=True)\n",
    "    q1_rewards+=run_rewards\n",
    "    q1_avgs.append(np.average(run_rewards))\n",
    "    q1_total_wins += sum(1 if rew ==1 else 0 for rew in run_rewards)\n",
    "\n",
    "print('Our agent won {} times'.format(q1_total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(q1_avgs)\n",
    "plt.xticks(ticks=range(len(q1_avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(q1_avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_qv = q1_q_learnt_player._Q_values\n",
    "print_Q_val_with_moves(_qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Q_val_with_moves(_qv, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Q_val_with_moves(_qv, only_vals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. \n",
    "#### Plot average reward for every 250 games during training. Does decreasing epsilon help training compared to having a fixed epsilon? What is the effect of n∗?\n",
    "Expected answer: A figure showing average reward over time for different values of n∗ (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_max_games = MAX_GAMES_DEFAULT\n",
    "q2_n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "q2_epoch_size = 250\n",
    "q2_rewards = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_avgs = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_M_opt = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_M_rand = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_total_wins = {n_star_q2: 0 for n_star_q2 in q2_n_stars}\n",
    "\n",
    "q2_players = {}\n",
    "\n",
    "q2_min_epsilon = 0.1\n",
    "q2_max_epsilon = 0.8\n",
    "def q2_calc_epsilon_factory(n_star_q2, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*q2_epoch_size + game_number_n\n",
    "            return max(q2_min_epsilon, q2_max_epsilon*(1-(real_game_number/n_star_q2)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q2 in q2_n_stars:\n",
    "    # epsilons.append([])\n",
    "    q2_starting_epsilon  = q2_calc_epsilon_factory(n_star_q2, q2_epoch_size, 0)(0)\n",
    "    q2_q_learnt_player = QLearntPlayer(_epsilon=q2_starting_epsilon)\n",
    "    q2_players[n_star_q2] = q2_q_learnt_player\n",
    "    print('Current n_star_q2 = {}'.format(n_star_q2))\n",
    "\n",
    "    for game_epoch in range(q2_max_games//q2_epoch_size):\n",
    "        q2_calc_epsilon = q2_calc_epsilon_factory(n_star_q2=n_star_q2, epoch_size=q2_epoch_size, game_epoch=game_epoch)\n",
    "        # print(game_epoch, q2_q_learnt_player._epsilon)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q2_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        q2_run_rewards = run_n_games_against_opt(_max_games_number=q2_epoch_size, _our_player=q2_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=q2_calc_epsilon, _update_q_values=True)\n",
    "        q2_avgs[n_star_q2].append(np.average(q2_run_rewards))\n",
    "        q2_rewards[n_star_q2] += q2_run_rewards\n",
    "        q2_total_wins[n_star_q2] += sum(1 if rew ==1 else 0 for rew in q2_run_rewards)\n",
    "\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q2_M_opt_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q2_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q2_M_opt[n_star_q2].append(np.average(q2_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q2_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q2_M_rand[n_star_q2].append(np.average(M_rand_rewards))\n",
    "\n",
    "print('won games per agent', q2_total_wins)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# avgs = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "# for x in range(0,q2_max_games, 250):\n",
    "#     lower_index = x\n",
    "#     upper_index = min(x+250, q2_max_games-1)\n",
    "#     for n_star_q2 in q2_n_stars:\n",
    "#         slice = q2_rewards[n_star_q2][lower_index:upper_index]\n",
    "#         avgs[n_star_q2].append(sum(slice)/len(slice))\n",
    "\n",
    "q2_data = pd.DataFrame(q2_avgs)\n",
    "q2_data.index.name = 'epochs by 250'\n",
    "q2_data.transpose().index.name = 'n_star_q2'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average q2_rewards per 250 epochs')\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# ax = plt.plot(q2_avgs)\n",
    "# plt.xticks(ticks=range(len(q2_avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs))])\n",
    "# plt.ylabel('Average reward per 250 episodes')\n",
    "# plt.xlabel('Episode (thousands)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_players[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = list(q2_players.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# avgs = {n_star: [] for n_star in n_stars}\n",
    "# for x in range(0,max_games, 250):\n",
    "#     lower_index = x\n",
    "#     upper_index = min(x+250, max_games-1)\n",
    "#     for n_star in n_stars:\n",
    "#         slice = rewards[n_star][lower_index:upper_index]\n",
    "#         avgs[n_star].append(sum(slice)/len(slice))\n",
    "\n",
    "# q2_data = pd.DataFrame(avgs)\n",
    "# q2_data.index.name = 'epochs by 250'\n",
    "# q2_data.transpose().index.name = 'n_star'\n",
    "# sns.set(rc={'figure.figsize':(24,12)})\n",
    "# g = sns.lineplot(data=q2_data)\n",
    "# g.set_ylabel('Average rewards per 250 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the same plot as in exe 1 to ensure results are the same\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(q2_avgs[1.0])\n",
    "plt.xticks(ticks=range(len(q2_avgs[1.0])), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(q2_avgs[1.0]))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n_star, rews) in q2_rewards.items():\n",
    "    print('{} won {} games'.format(n_star, sum(1 if rew ==1 else 0 for rew in rews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q2_rewards[1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "*Does decreasing epsilon help with training compared to fix epsilon?*\n",
    "\n",
    "Surprisingly, it seems to only help very little. Perhaps this is because the game is relatively simple and has few states and in consequence the minimum epsilon value of $\\varepsilon=0$.1 is large enough for enough exploration to happen, but notice that $n^*=1$ (which means virtually no decrease and instead keeping epsilon stable at 0.1 for all games) seems to perform as well as having 14, 200, or almost 3000 exploratory games. \n",
    "\n",
    "*What is the effect of n\\*?*\n",
    "\n",
    "The effect seems to be quite limitted: unsurprisingly, if $n^*=40000$, the algorithm performs very poorly. This makes sense, because such algorithms plays the 20000th game with $\\varepsilon= (1- \\frac{20000}{40000})*0.8 = 0.4$, i.e. it plays 40% of the moves randomly in the last game (and even more before). All the other values converge to eps = 0.1 by the end of the simulation and they are thus more comparable. It seems that the difference between the remaining algorithms is very small: indeed, perhaps $n^*=2828$ performs best, it seems to be most consistently on top and it wins most games overall (9850 as opposed to 9583 won by $\\varepsilon=200$), but the graph is very noisy and it is hard to tell if this difference is entirely due to better training which explores more or if it is largely just an artifact of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q2_M_opt_df = pd.DataFrame(q2_M_opt)\n",
    "q2_M_opt_df.index.name = 'epochs by 250'\n",
    "q2_M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q2_M_rand_df = pd.DataFrame(q2_M_rand)\n",
    "q2_M_rand_df.index.name = 'epochs by 250'\n",
    "q2_M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_M_rand_df)\n",
    "g.set_ylabel('q2_M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "*Describe the differences and similarities between the curves and the one in the previous question*\n",
    "\n",
    "Here we can see all values of $n^*$ quickly reach almost optimal performance against the optimal player. This is perhaps because often one good move can ruin an opponent's winning chances. It takes much longer to achieve optimal play against a random oponent, perhaps because again, we need to play all moves perfectly to win, but only 1 correct move can prevent failure and thus it is easier to learn to defend and not-lose against a good player than to beat a poor one. Here we again see the $n^*=40000$ player performing rather poorly just like we did in the previous exercise, but the gap is not as large as before. The best performing player is again $n^*=2828$ but we can see the data is again noisy with $n^*=1$ or $n^*=200$ performing best in some sets of 250 epochs. **TODO: It is a bit weird we perform so well against M_opt from the beginning, perhaps we should investigate this if there is a bug (I dont see where there could be but the result is suspicious)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "Choose the best value of $n^∗$ that you found in the previous section. Run Q-learning against Opt($\\epsilon_{opt}$) for\n",
    "different values of $\\epsilon_{opt}$ for 20’000 games – switch the 1st player after every game. Choose several values\n",
    "of $\\epsilon_{opt}$ from a reasonably wide interval between 0 to 1 – particularly, include $\\epsilon_{opt}$ = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents\n",
    "– for each value of $\\epsilon_{opt}$. Plot $M_{opt}$ and $M_{rand}$ over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon_{opt}$ (caption length\n",
    "< 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_star = 2828.427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_max_games = MAX_GAMES_DEFAULT\n",
    "q4_epoch_size = 250\n",
    "\n",
    "q4_eps_opts = np.linspace(0,1,num=5)\n",
    "\n",
    "q4_rewards_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "q4_M_opt_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "q4_M_rand_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "\n",
    "q4_min_epsilon = 0.1\n",
    "q4_max_epsilon = 0.8\n",
    "\n",
    "\n",
    "def q4_calc_epsilon_factory_eps(epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q4_min_epsilon, q4_max_epsilon*(1-(real_game_number/best_n_star)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for eps_opt in q4_eps_opts:\n",
    "    q4_starting_epsilon  = q4_calc_epsilon_factory_eps(q4_epoch_size, 0)(0)\n",
    "    q4_q_learnt_player = QLearntPlayer(_epsilon=q4_starting_epsilon)\n",
    "    print('Current eps_opt = {}'.format(eps_opt))\n",
    "\n",
    "    for game_epoch in range(q4_max_games//q4_epoch_size):\n",
    "        q4_calc_epsilon = q4_calc_epsilon_factory_eps(epoch_size=q4_epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q4_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 4)\n",
    "        q4_run_rewards = run_n_games_against_opt(_max_games_number=q4_epoch_size, _our_player=q4_q_learnt_player, _opponent_epsilon=eps_opt, \\\n",
    "            _our_player_new_game_epsilon=q4_calc_epsilon, _update_q_values=True)\n",
    "        q4_rewards_eps[eps_opt] += q4_run_rewards\n",
    "\n",
    "        # Run 500 games for q4_M_opt_eps calculation\n",
    "        q4_M_opt_eps_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q4_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q4_M_opt_eps[eps_opt].append(np.average(q4_M_opt_eps_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q4_M_rand_eps calculation\n",
    "        q4_M_rand_eps_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q4_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q4_M_rand_eps[eps_opt].append(np.average(q4_M_rand_eps_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q4_M_opt_eps_df = pd.DataFrame(q4_M_opt_eps)\n",
    "q4_M_opt_eps_df.index.name = 'epochs by 250'\n",
    "q4_M_opt_eps_df.transpose().index.name = 'eps'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q4_M_opt_eps_df)\n",
    "g.set_ylabel('M_opt_eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q4_M_rand_eps_df = pd.DataFrame(q4_M_rand_eps)\n",
    "q4_M_rand_eps_df.index.name = 'epochs by 250'\n",
    "q4_M_rand_eps_df.transpose().index.name = 'eps'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q4_M_rand_eps_df)\n",
    "g.set_ylabel('M_rand_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (answered). \n",
    "After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents\n",
    "– for each value of $\\epsilon_{opt}$. Plot $M_{opt}$ and $M_{rand}$ over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon_{opt}$ (caption length\n",
    "< 250 words).\n",
    "\n",
    "The most interesting observation is the discrepancy between performance of $\\varepsilon=0$ in M_opt and M_rand (and the same but reversed for $\\varepsilon=1$). This is due to the fact that when we play a perfect oponent ($\\varepsilon=0$) only, we learn to defend perfectly (hence very good performance in M_opt). However, we never get into winning positions, hence when we are then faced with a random opponent, our performance is only slightly better than 0 as we also play randomly in the winning positions hence our Q-values are 0 for all moves as we have never encountered these positions. It is similar in reverse for ($\\varepsilon=1$) which never gets properly punished for going into bad positions and it consequently fails when playing against the optimal opponent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best M_opt:', q4_M_opt_eps_df.max())\n",
    "print('\\n')\n",
    "print('Best M_rand', q4_M_rand_eps_df.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q5 (answer)\n",
    "What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?\n",
    "\n",
    "The highest value of $M_{opt}$ is achieved by all $\\varepsilon$, except $\\varepsilon=1$ and it is the value 0 which is the best that we can hope for against  $M_{opt}$. The highest value of $M_{rand}$ is near $0.910$ achieved by  $\\varepsilon=0.75$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 (answer)\n",
    "*Question 6. (Theory) Assume that Agent 1 learns by playing against $\\text{Opt}(0)$ and find the optimal Q-\n",
    "values $Q_1(s, a)$. In addition, assume that Agent 2 learns by playing against $\\text{Opt}(1)$ and find the optimal\n",
    "Q-values $Q_2(s, a)$. Do $Q_1(s, a)$ and $Q_2(s, a)$ have the same values? Justify your answer. (answer length\n",
    "< 150 words)*\n",
    "\n",
    "No, they will not have the same values. This is because if we play an optimal agent, we will never win and thus never observe a positive reward. Therefore all Q-values will be at best 0. However, playing against a random oponent, we will definitely win sometimes and as we will get closer and closer to convergence, we will play better and better and win more and more. We will therefore definitely observe positive rewards at times. Hence, the Q-values will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Learning by practice\n",
    "In this section, your are supposed to ask whether Q-learning can learn to play Tic Tac Toe by only\n",
    "playing against itself. For different values of $\\epsilon \\in [0, 1)$, run a Q-learning agent against itself for 20’000\n",
    "games – i.e. both players use the same set of Q-values and update the same set of Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (code below)\n",
    "*Question 7. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_self(_max_games_number, _our_player, _our_player_new_game_epsilon, _update_q_values, _progress_print=None, _throw_error_on_illegal_moves = True):\n",
    "    _rewards = {'our_player': [None for _ in range(_max_games_number)],'opponent': [None for _ in range(_max_games_number)]}\n",
    "    _turns = ['X','O']\n",
    "    if isinstance(_our_player,QLearntPlayer):\n",
    "        # Added this branch to make this work for DQN also. \n",
    "        opponent = QLearntPlayer(_our_player._epsilon,_our_player._discount_rate_gamma, _our_player._learning_rate_alpha)\n",
    "\n",
    "        # Ensure they share Q_values... I think this should just work\n",
    "        opponent._Q_values = _our_player._Q_values\n",
    "\n",
    "    # ! TODO: Make this also work for DQN_Player\n",
    "    # if isinstance(_our_player, DQN_Player):\n",
    "        # ! Init DQN opponent here.\n",
    "        # ! THere are problems with defining this function this way as when it runs,\n",
    "        # DQN player is not yet defined. Perhaps these two branches could be unified, but I leave this to\n",
    "        # you Alex:)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        if _progress_print and game % _progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        _current_grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent = opponent.prepare_new_game_(_turns[0])\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent._player != _our_player._player\n",
    "        _our_player._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "        opponent._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent._player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.choose_move_(_current_grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(_current_grid)\n",
    "\n",
    "            if (not _throw_error_on_illegal_moves) and (chosen_move not in get_empty_positions_from_grid(_current_grid)):\n",
    "                # Added this branch to enable this function to be used for DQN also (jl, may 7)\n",
    "                _current_grid = None\n",
    "                _end = True\n",
    "                # Whoever makes an illegal move loses, but should the other player observe reward\n",
    "                # or should they just see reward as 0? Not sure (jl, May 7)\n",
    "                _our_player_reward = 1 if opponent_turn else -1\n",
    "                _opponent_reward = -1 if opponent_turn else 1\n",
    "\n",
    "            else:\n",
    "                _current_grid, _end, winner = env.step(chosen_move, print_grid=False)\n",
    "                _our_player_reward = env.reward(_our_player._player)\n",
    "                _opponent_reward = env.reward(opponent._player)\n",
    "\n",
    "            if _end:\n",
    "                _update_q_values and _our_player.update_q_values_(_current_grid, game_over = _end, _reward=_our_player_reward)\n",
    "                _update_q_values and opponent.update_q_values_(_current_grid, game_over = _end, _reward=_opponent_reward)\n",
    "                _rewards['our_player'][game] = _our_player_reward\n",
    "                _rewards['opponent'][game] = _opponent_reward\n",
    "                break\n",
    "            elif turn !=0 and _update_q_values:\n",
    "                if opponent_turn:\n",
    "                    _our_player.update_q_values_(_current_grid, game_over = _end, _reward=_our_player_reward)\n",
    "                else:\n",
    "                    opponent.update_q_values_(_current_grid, game_over = _end, _reward=_opponent_reward)\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7_max_games = MAX_GAMES_DEFAULT\n",
    "q7_epoch_size = 250\n",
    "\n",
    "q7_eps_selfs = np.linspace(0,0.99,num=5)\n",
    "\n",
    "q7_M_opt_self = {eps_opt: [] for eps_opt in q7_eps_selfs}\n",
    "q7_M_rand_self = {eps_opt: [] for eps_opt in q7_eps_selfs}\n",
    "\n",
    "q7_min_epsilon = 0.1\n",
    "q7_max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "for eps_s in q7_eps_selfs:\n",
    "    q7_q_learnt_player = QLearntPlayer(_epsilon=q7_min_epsilon)\n",
    "    print('Current eps_s = {}'.format(eps_s))\n",
    "\n",
    "    for game_epoch in range(q7_max_games//q7_epoch_size):\n",
    "        q7_calc_epsilon = lambda game_number_n: eps_s\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q7_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 7)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=q7_epoch_size, _our_player=q7_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=q7_calc_epsilon, _update_q_values=True)\n",
    "\n",
    "        # Run 500 games for q7_M_opt_self calculation\n",
    "        q7_M_opt_self_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q7_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q7_M_opt_self[eps_s].append(np.average(q7_M_opt_self_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q7_M_rand_self calculation\n",
    "        q7_M_rand_self_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q7_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q7_M_rand_self[eps_s].append(np.average(q7_M_rand_self_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q7_M_opt_self_df = pd.DataFrame(q7_M_opt_self)\n",
    "q7_M_opt_self_df.index.name = 'epochs by 250'\n",
    "q7_M_opt_self_df.transpose().index.name = 'epsilon'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q7_M_opt_self_df)\n",
    "g.set_ylabel('q7_M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q7_M_opt_self_df = pd.DataFrame(q7_M_opt_self[q7_eps_selfs[1]])\n",
    "q7_M_opt_self_df.index.name = 'epochs by 250'\n",
    "q7_M_opt_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q7_M_opt_self_df)\n",
    "g.set_ylabel('q7_M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q7_M_rand_self_df = pd.DataFrame(q7_M_rand_self)\n",
    "q7_M_rand_self_df.index.name = 'epochs by 250'\n",
    "q7_M_rand_self_df.transpose().index.name = 'epsilon'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q7_M_rand_self_df)\n",
    "g.set_ylabel('q7_M_rand_self')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (answer)\n",
    "*Question 7. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\varepsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\varepsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\varepsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*\n",
    "\n",
    "\n",
    "#### Does the agent learn to play Tic Tac Toe?\n",
    "Indeed the agent does learn to play Tic Tac Toe as we can see from the improving performance against both $M_{opt}$ and $M_{rand}$. It never achieves perfect performance against $M_{opt}$ like previous models but it achieves pretty comparable performance against $M_{rand}$.\n",
    "\n",
    "\n",
    "#### What is the effect of $\\varepsilon$?\n",
    "We observe similar effect to what we saw before with setting different $\\varepsilon$ for optimal oponent: $\\varepsilon \\rightarrow 1$ defends quite poorly against optimal player while $\\varepsilon=0$ never learns. This makes sense because $\\varepsilon=0$ will not do any exploration and consequently, its Q-values will be 0 in most positions as it will never have seen them during training. Other values of $\\varepsilon$ all achieve similar performance and differ mostly in the number of epochs it takes them to get to it, the optimal $\\varepsilon$ seems to be around $\\sim 0.5$ out of the values we tried.\n",
    "\n",
    "**TODO:** The graph against M_opt looks a bit weird, why the large jumps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (code below)\n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^∗$ – instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^∗$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^∗$ (caption\n",
    "length < 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8_max_games = MAX_GAMES_DEFAULT\n",
    "q8_n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "q8_rewards_self_n_stars = {n_star_q8: {'our_player': [], 'opponent':[]} for n_star_q8 in q8_n_stars}\n",
    "q8_M_opt_self_n_stars = {n_star_q8: [] for n_star_q8 in q8_n_stars}\n",
    "q8_M_rand_self_n_stars = {n_star_q8: [] for n_star_q8 in q8_n_stars}\n",
    "\n",
    "q8_min_epsilon = 0.1\n",
    "q8_max_epsilon = 0.8\n",
    "def q8_calc_epsilon_factory(n_star_q8, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q8_min_epsilon, q8_max_epsilon*(1-(real_game_number/n_star_q8)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q8 in q8_n_stars:\n",
    "    q8_q_learnt_player = QLearntPlayer(_epsilon=q8_max_epsilon)\n",
    "    print('Current n_star_q8 = {}'.format(n_star_q8))\n",
    "\n",
    "    for game_epoch in range(q8_max_games//epoch_size):\n",
    "        q8_calc_epsilon = q8_calc_epsilon_factory(n_star_q8=n_star_q8, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 8)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=epoch_size, _our_player=q8_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=q8_calc_epsilon, _update_q_values=True)\n",
    "        q8_rewards_self_n_stars[n_star_q8]['our_player'] += run_rewards['our_player']\n",
    "        q8_rewards_self_n_stars[n_star_q8]['opponent'] += run_rewards['opponent']\n",
    "\n",
    "        # Run 500 games for q8_M_opt_self_n_stars calculation\n",
    "        q8_M_opt_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q8_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q8_M_opt_self_n_stars[n_star_q8].append(np.average(q8_M_opt_self_n_stars_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q8_M_rand_self_n_stars calculation\n",
    "        q8_M_rand_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q8_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q8_M_rand_self_n_stars[n_star_q8].append(np.average(q8_M_rand_self_n_stars_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q8_M_rand_self_n_stars_df = pd.DataFrame(q8_M_rand_self_n_stars)\n",
    "q8_M_rand_self_n_stars_df.index.name = 'epochs by 250'\n",
    "q8_M_rand_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q8_M_rand_self_n_stars_df)\n",
    "g.set_ylabel('q8_M_rand_self_n_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "q8_M_opt_self_n_stars_df = pd.DataFrame(q8_M_opt_self_n_stars)\n",
    "q8_M_opt_self_n_stars_df.index.name = 'epochs by 250'\n",
    "q8_M_opt_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q8_M_opt_self_n_stars_df)\n",
    "g.set_ylabel('q8_M_opt_self_n_stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (answer)\n",
    "For rest of this section, use $\\varepsilon(n)$ in Equation 1 with different values of $n^∗$ – instead of fixing $\\varepsilon$.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\varepsilon$ help training compared to having a fixed $\\varepsilon$? What is the effect of $n^∗$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^∗$ (caption\n",
    "length < 100 words).\n",
    "\n",
    "#### Does decreasing $\\varepsilon$ help training compared to having a fixed $\\varepsilon$? What is the effect of $n^∗$?\n",
    "Here, decreasing epsilon seems to help massively especially when compared with its effect without left-play. Interestingly, it seems the higher $n^∗$ the better (unlike before). This makes sense because $n^∗=40000$ means that $\\varepsilon$ will spend a large chunk of its time around $\\sim 0.5$ which we discovered as the best value for a fixed epsilon in the previous step. Here, a variable $\\varepsilon$ seems to perform even better, especially against $M_{opt}$.\n",
    "\n",
    "\n",
    "**TODO:** A little strange that we get optimal perf against M_opt but then we lose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best M_opt:', q8_M_opt_self_n_stars_df.max())\n",
    "print('\\n')\n",
    "print('Best M_rand', q8_M_rand_self_n_stars_df.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "*Question 9. What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?*\n",
    "\n",
    "The best value for $M_{opt}$ was **0** achieved by $n^*=40000$ and the best value for $M_{rand}$ was **0.95** achieved again by $n^*=40000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n",
    "*Question 10. For three board arrangements (i.e. states s), visualize Q-values of available actions (e.g.\n",
    "using heat maps). Does the result make sense? Did the agent learn the game well?\n",
    "Expected answer: A figure with 3 subplots of 3 different states with Q-values shown at available actions\n",
    "(caption length < 200 words).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_to_display = random.sample(list(q8_q_learnt_player._Q_values.items()), 3)\n",
    "positions_to_display = [(grid_to_string(k), v) for (k,v) in positions_to_display]\n",
    "positions_to_display[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "heatmap_data = []\n",
    "current_pos = positions_to_display[1]\n",
    "for i in range(9):\n",
    "    pos = (int(i/3), i % 3)\n",
    "    if current_pos[1].get(pos):\n",
    "        heatmap_data.append(current_pos[1].get(pos))\n",
    "    else:\n",
    "        heatmap_data.append(0)\n",
    "print(current_pos[0])\n",
    "sns.heatmap(np.array(heatmap_data).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "heatmap_data = []\n",
    "current_pos = positions_to_display[2]\n",
    "for i in range(9):\n",
    "    pos = (int(i/3), i % 3)\n",
    "    if current_pos[1].get(pos):\n",
    "        heatmap_data.append(current_pos[1].get(pos))\n",
    "    else:\n",
    "        heatmap_data.append(0)\n",
    "print(current_pos[0])\n",
    "sns.heatmap(np.array(heatmap_data).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10 (answer)\n",
    "#### Does the result make sense? Did the agent learn the game well?\n",
    "We sampled 3 randoms states from the best agent taught by selfplay ($n^* = 40000$). The heatmaps show the agent has indeed learnt to play the game well – the best move always has the highest Q-value and winning moves have Q-values above 0 while losing moves have Q-values below 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep Q-Learning\n",
    "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch\n",
    "again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of\n",
    "Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is\n",
    "to approximate Q-values by a neural network instead of a look-up table as in Tabular Q-learning. For\n",
    "implementation, you can use ideas from the DQN tutorials of Keras and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Learning from experts\n",
    "Implement the DQN algorithm. To check the algorithm, run a DQN agent with a fixed and arbitrary\n",
    "$\\epsilon \\in [0,1)$ against Opt(0.5) for 20’000 games – switch the 1st player after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "*Plot average reward and average training loss for every 250 games during training. Does\n",
    "the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure with two subplots (caption length $<$ 50 words). Specify your choice of $\\epsilon$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We use $\\epsilon = 0.05$ as that is the value that Mnih at el (2015) use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def state_to_torch(game_state: np.ndarray):\n",
    "    torch_no_batch =  torch.cat((torch.from_numpy((game_state == 1).astype(np.float32)).view(1,3,3,1), torch.from_numpy((game_state == -1).astype(np.float32)).view(1,3,3,1)), dim=3).to(device)\n",
    "    return torch_no_batch\n",
    "\n",
    "def game_state_converts_to_pytorch_correctly():\n",
    "    game_state = np.array([ [ 1.,  1.,  1.], \\\n",
    "                            [ 1.,  0., -1.], \\\n",
    "                            [-1., -1.,  1.]])\n",
    "    torch_repre = state_to_torch(game_state).view(3,3,2)\n",
    "    expected_our_positions = torch.from_numpy((game_state==1).astype(int))\n",
    "    torch_repre_our_positions = torch_repre[:,:,0]\n",
    "    assert (expected_our_positions == torch_repre_our_positions).all()\n",
    "    expected_opponent_positions = torch.from_numpy((game_state==-1).astype(int))\n",
    "    torch_repre_opponent_positions = torch_repre[:,:,1]\n",
    "    assert (expected_opponent_positions == torch_repre_opponent_positions).all()\n",
    "    return True\n",
    "\n",
    "assert game_state_converts_to_pytorch_correctly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell's code is largely taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html?highlight=huber\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # \" Once a bounded length deque is full, when new items are added, a corresponding number\n",
    "        # of items are discarded from the opposite end. \" (https://docs.python.org/3/library/collections.html#collections.deque)\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, _new_memory: Transition):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(_new_memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net =  nn.Sequential(\n",
    "            nn.Linear(18, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 9))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        batch_size = inp.size(0)\n",
    "        input_in_1D = inp.view(batch_size,-1)\n",
    "        output = self.net(input_in_1D)\n",
    "        return output\n",
    "\n",
    "summary(DQNet().to(device), input_size=(3,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_tensor = torch.tensor([0.1,0.2]  +5*[0] + [-0.3,-0.4]).view(9,-1)\n",
    "eg_tensor.numpy().reshape(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Player(BasePlayer):\n",
    "    INDEX_TO_POSITIONS = BasePlayer().get_empty_positions(TictactoeEnv().grid)\n",
    "    POSITIONS_TO_INDEX = {position: index for index, position in enumerate(INDEX_TO_POSITIONS)}\n",
    "    LEARNING_RATE = 5 * 10e-4 # Given in the instructions PDF\n",
    "    DISCOUNT_RATE_GAMMA = 0.99\n",
    "    DEFAULT_BATCH_SIZE = 64\n",
    "    DEFAULT_BUFFER_SIZE = 10_000\n",
    "    TARGET_NEW_UPDATE_FREQUENCY = 500\n",
    "\n",
    "    def __init__(self, _epsilon: float, _batch_size = DEFAULT_BATCH_SIZE, _replay_buffer_size = DEFAULT_BUFFER_SIZE):\n",
    "        self._policy_net = DQNet()\n",
    "        self._target_net = DQNet()\n",
    "        self._target_net.load_state_dict(self._policy_net.state_dict())\n",
    "        self._replay_buffer = ReplayMemory(_replay_buffer_size)\n",
    "        self._optimizer = torch.optim.Adam(self._policy_net.parameters(), lr=DQN_Player.LEARNING_RATE)\n",
    "        self._criterion = nn.SmoothL1Loss() # \"When delta is set to 1, this loss is equivalent to SmoothL1Loss.\" (PyTorch HuberLoss documentation)\n",
    "        self._epsilon = _epsilon\n",
    "        self._latest_memory = {'state': None, 'action': None, 'next_state': None, 'reward': None}\n",
    "        self._player = None # 'X' or 'O'\n",
    "        self._game_count = 0\n",
    "        self._batch_size = _batch_size\n",
    "        self._replay_buffer_size = _replay_buffer_size\n",
    "\n",
    "    def prepare_new_game_(self, _player):\n",
    "        # These assertions make sense when we are learning but they get broken\n",
    "        # during M_opt and M_rand calculations\n",
    "        # assert self._latest_memory['state'] is None\n",
    "        # assert self._latest_memory['action'] is None\n",
    "        # assert self._latest_memory['next_state'] is None\n",
    "        # assert self._latest_memory['reward'] is None\n",
    "        assert _player == 'X' or _player == 'O'\n",
    "        self._player = _player\n",
    "        self._game_count += 1\n",
    "        return self\n",
    "\n",
    "    def get_q_values_from_network(self, _torch_grid):\n",
    "        with torch.no_grad():\n",
    "            net_q_vals = self._policy_net(_torch_grid)\n",
    "            # This is slow and it would be easier to just take the max but I implement it like this for easier debugging\n",
    "            # Also this is consistent with previous Q-value representation, which will allow us to reuse debugging functions\n",
    "            numpy_q_vals = net_q_vals.numpy().reshape(9)\n",
    "            dict_q_vals = {move: numpy_q_vals[move_index] for (move_index, move) in enumerate(DQN_Player.INDEX_TO_POSITIONS)} \n",
    "            return dict_q_vals\n",
    "\n",
    "    def get_max_val_action(self, _q_values):\n",
    "        return max(_q_values, key=_q_values.get)\n",
    "\n",
    "    def choose_move_(self, _grid):\n",
    "        torch_grid = state_to_torch(_grid)\n",
    "\n",
    "        random_sample = random.random()\n",
    "        play_best_move = random_sample >= self._epsilon\n",
    "        if play_best_move:\n",
    "            q_values = self.get_q_values_from_network(_torch_grid=torch_grid)\n",
    "            chosen_move = self.get_max_val_action(_q_values=q_values)\n",
    "        else:\n",
    "            possible_moves = self.get_empty_positions(_grid)\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "\n",
    "        # These assertions make sense when we are learning but they get broken\n",
    "        # during M_opt and M_rand calculations\n",
    "        # assert self._latest_memory['state'] is None\n",
    "        # assert self._latest_memory['action'] is None\n",
    "\n",
    "        self._latest_memory['state'] = torch_grid\n",
    "        self._latest_memory['action'] = torch.tensor([DQN_Player.POSITIONS_TO_INDEX[chosen_move]])\n",
    "\n",
    "        if DEBUG:\n",
    "            if not play_best_move:\n",
    "                q_values = self.get_q_values_from_network(_torch_grid=torch_grid) # ensure q_values are defined\n",
    "            print('-----------------------------------')\n",
    "            print('Current position: ', '\\n' + grid_to_string(_grid, False))\n",
    "            print('Current Q-vals', q_values)\n",
    "            print('Random sample ', random_sample, ' _epsilon ', self._epsilon, ' hence I chose ', \\\n",
    "                '*best*' if play_best_move else '*random*', ' move: ', chosen_move )\n",
    "            print('-----------------------------------')\n",
    "\n",
    "        return chosen_move\n",
    "\n",
    "    def reset_latest_memory_(self):\n",
    "        self._latest_memory['state'] = None\n",
    "        self._latest_memory['action'] = None\n",
    "        self._latest_memory['next_state'] = None\n",
    "        self._latest_memory['reward'] = None\n",
    "\n",
    "    def create_memory_(self, _next_state, _reward):\n",
    "        # assert self._latest_memory['state'] is not None\n",
    "        # assert self._latest_memory['action'] is not None\n",
    "        # assert self._latest_memory['next_state'] is None\n",
    "        # assert self._latest_memory['reward'] is None\n",
    "\n",
    "        self._latest_memory['next_state'] = _next_state\n",
    "        self._latest_memory['reward'] = torch.tensor([_reward])\n",
    "        new_memory = Transition(**self._latest_memory)\n",
    "        self._replay_buffer.push(new_memory)\n",
    "        self.reset_latest_memory_()\n",
    "\n",
    "    def learn_from_memories_(self):\n",
    "        if len(self._replay_buffer) < self._batch_size:\n",
    "            return\n",
    "        replay_memories = self._replay_buffer.sample(self._batch_size)\n",
    "\n",
    "        # next few lines inspired by https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training-loop\n",
    "        batch = Transition(*zip(*replay_memories))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s != '', \\\n",
    "            batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = state_batch[non_final_mask]\n",
    "\n",
    "        _temp = self._policy_net(state_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pre_move_q_vals = _temp.gather(1, action_batch.view(action_batch.size(0),1))\n",
    "        # print(action_batch)\n",
    "\n",
    "\n",
    "\n",
    "        post_move_q_vals = torch.zeros(self._batch_size)\n",
    "\n",
    "        # Don't call net if there are no non_final states in the batch\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            post_move_q_vals[non_final_mask] = self._target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_post_move_q_vals = (post_move_q_vals * self.DISCOUNT_RATE_GAMMA) + reward_batch\n",
    "\n",
    "        # print(pre_move_q_vals.shape)\n",
    "        loss = self._criterion(pre_move_q_vals, expected_post_move_q_vals.unsqueeze(1))\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        if self._game_count % self.TARGET_NEW_UPDATE_FREQUENCY == 0:\n",
    "            self._target_net.load_state_dict(self._policy_net.state_dict())\n",
    "\n",
    "    def update_q_values_(self, new_grid, game_over, _reward):\n",
    "        \"\"\"\n",
    "        same signature and name as in QLearntPlayer in order to keep the same interface,\n",
    "        even though another name would perhaps be more appropriate here\n",
    "        \"\"\"\n",
    "        # If the next state is a final state, save '' instead: we need to signify the state is final\n",
    "        # as the target network cannot predict the q-value of best action there since in a final state,\n",
    "        # no action can be taken (we just give it Q-val of 0 instead)\n",
    "        next_state = state_to_torch(new_grid) if not game_over else ''\n",
    "        self.create_memory_(_next_state=next_state, _reward=_reward)\n",
    "        self.learn_from_memories_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q11_epsilon = 0.1 # Chosen because they use this in q2 so this will allow us to nicely compare\n",
    "\n",
    "q11_max_games = MAX_GAMES_DEFAULT\n",
    "q11_dqn_player = DQN_Player(_epsilon=q11_epsilon)\n",
    "q11_epoch_size = 250\n",
    "q11_rewards = []\n",
    "q11_avgs = []\n",
    "q11_M_opt = []\n",
    "q11_M_rand = []\n",
    "q11_total_wins = 0\n",
    "\n",
    "for game_epoch in range(q11_max_games//q11_epoch_size):\n",
    "    if game_epoch % 20 == 0:\n",
    "        print('Game ', game_epoch*q11_epoch_size, ' begins.')\n",
    "\n",
    "    # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "    q11_run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q11_dqn_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q11_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "    q11_avgs.append(np.average(q11_run_rewards))\n",
    "    q11_rewards += q11_run_rewards\n",
    "    q11_total_wins += sum(1 if rew ==1 else 0 for rew in q11_run_rewards)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q11_M_opt_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q11_dqn_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q11_M_opt.append(np.average(q11_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q11_dqn_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q11_M_rand.append(np.average(M_rand_rewards))\n",
    "\n",
    "print('won games per agent', q11_total_wins)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print('Our agent won {} times'.format(q11_total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(q11_avgs)\n",
    "plt.xticks(ticks=range(len(q11_avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(q11_avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q11_M_opt_df = pd.DataFrame(q11_M_opt)\n",
    "q11_M_opt_df.index.name = 'epochs by 250'\n",
    "q11_M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q11_M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q11_M_rand_df = pd.DataFrame(q11_M_rand)\n",
    "q11_M_rand_df.index.name = 'epochs by 250'\n",
    "q11_M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q11_M_rand_df)\n",
    "g.set_ylabel('q11_M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q12_epsilon = 0.1 # Chosen because they use this in q2 so this will allow us to nicely compare\n",
    "\n",
    "q12_max_games = MAX_GAMES_DEFAULT\n",
    "q12_dqn_player = DQN_Player(_epsilon=q12_epsilon, _batch_size=1, _replay_buffer_size=1)\n",
    "q12_epoch_size = 250\n",
    "q12_rewards = []\n",
    "q12_avgs = []\n",
    "q12_M_opt = []\n",
    "q12_M_rand = []\n",
    "q12_total_wins = 0\n",
    "\n",
    "for game_epoch in range(q12_max_games//q12_epoch_size):\n",
    "    if game_epoch % 20 == 0:\n",
    "        print('Game ', game_epoch*q12_epoch_size, ' begins.')\n",
    "\n",
    "    # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "    q12_run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q12_dqn_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q12_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "    q12_avgs.append(np.average(q12_run_rewards))\n",
    "    q12_rewards += q12_run_rewards\n",
    "    q12_total_wins += sum(1 if rew ==1 else 0 for rew in q12_run_rewards)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q12_M_opt_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q12_dqn_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q12_M_opt.append(np.average(q12_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=500, _our_player=q12_dqn_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q12_M_rand.append(np.average(M_rand_rewards))\n",
    "\n",
    "print('won games per agent', q12_total_wins)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print('Our agent won {} times'.format(q12_total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(q12_avgs)\n",
    "plt.xticks(ticks=range(len(q12_avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(q12_avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q12_M_opt_df = pd.DataFrame(q12_M_opt)\n",
    "q12_M_opt_df.index.name = 'epochs by 250'\n",
    "q12_M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q12_M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q12_M_rand_df = pd.DataFrame(q12_M_rand)\n",
    "q12_M_rand_df.index.name = 'epochs by 250'\n",
    "q12_M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q12_M_rand_df)\n",
    "g.set_ylabel('q12_M_rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f371659e15671c4bf19c781ff73734bc385c323133a3685f5b4f679a2745d0"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
