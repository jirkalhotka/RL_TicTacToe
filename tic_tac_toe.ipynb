{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "env = TictactoeEnv()\n",
    "Turns = np.array(['X','O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against\n",
    "Opt(eps_opt) for some eps_opt ∈ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm,\n",
    "run a Q-learning agent, with a fixed and arbitrary eps ∈ [0, 1), against Opt(0.5) for 20’000 games – switch\n",
    "the 1st player after every game.\n",
    "Question 1. Plot average reward for every 250 games during training – i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_string(bts, is_buffer = True):\n",
    "        _grid = np.reshape(np.frombuffer(bts),(3,3)) if is_buffer else bts\n",
    "        str_rep = ''\n",
    "        value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "        for i in range(3):\n",
    "            str_rep +='|'\n",
    "            for j in range(3):\n",
    "                str_rep += value2player[int(_grid[i,j])] + (' ' if j<2 else '')\n",
    "            str_rep+='|\\n'\n",
    "        str_rep+='\\n'\n",
    "        return str_rep\n",
    "\n",
    "def print_Q_val_with_moves(q_vals, descending=False, only_vals = None):\n",
    "    qv_temp = {grid_to_string(k): q_vals[k] for k in q_vals if any(map(lambda a: q_vals[k][a] != 0, q_vals[k]))}.items()\n",
    "    count_free_squares = lambda grid: sum([x=='-' for x in grid])\n",
    "    if only_vals is not None:\n",
    "        qv_temp = list(filter(lambda k: count_free_squares(k[0]) == only_vals,qv_temp))\n",
    "    for (i,j) in sorted(qv_temp, key=lambda k: count_free_squares(k[0]), reverse=descending):\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# desparate debugging\n",
    "_epsilons = []\n",
    "\n",
    "class BasePlayer:\n",
    "    def get_empty_positions(self, _grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if _grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "\n",
    "    def hash_grid(self, _grid: np.ndarray):\n",
    "        return _grid.tobytes()\n",
    "\n",
    "class QLearntPlayer(BasePlayer):\n",
    "    def __init__(self, _game_env: TictactoeEnv, _epsilon: float, _discount_rate_gamma = 0.99, _learning_rate_alpha = 0.05):\n",
    "        super()\n",
    "        self._game_env = _game_env\n",
    "        self._epsilon = _epsilon\n",
    "        self._discount_rate_gamma = _discount_rate_gamma\n",
    "        self._learning_rate_alpha = _learning_rate_alpha\n",
    "        self._Q_values = {}\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        self._player = None # 'X' or 'O'\n",
    "\n",
    "    def prepare_new_game_(self, _player):\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        assert _player == 'X' or _player == 'O'\n",
    "        self._player = _player\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, _possible_moves, _grid_hash):\n",
    "        if len(_possible_moves) == 0:\n",
    "            q_val = self._Q_values[_grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        # Shuffle moves to prevent bias towards choosing the first thing in the list\n",
    "        # This is important especially at the beginning when all the q-vals are 0\n",
    "        # And we are therefore biased towards choosing the first avaliable move, e.g. (0,0) in the\n",
    "        # starting position\n",
    "        random.shuffle(_possible_moves) \n",
    "        return max(_possible_moves, key=self._Q_values[_grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, _grid_hash, _possible_moves):\n",
    "        if _grid_hash not in self._Q_values:\n",
    "            self._Q_values[_grid_hash] = {} if len(_possible_moves) > 0 else {'': 0}\n",
    "        for mv in _possible_moves:\n",
    "            if mv not in self._Q_values[_grid_hash]: self._Q_values[_grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, _grid):\n",
    "        grid_hash = self.hash_grid(_grid)\n",
    "        # Get moves\n",
    "        possible_moves = self.get_empty_positions(_grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        random_sample = random.random()\n",
    "        play_best_move = random_sample >= self._epsilon\n",
    "        if play_best_move:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self._curr_grid = _grid\n",
    "        self._curr_move = chosen_move\n",
    "        if DEBUG:\n",
    "            print('-----------------------------------')\n",
    "            print('Current position: ', '\\n' + grid_to_string(_grid, False))\n",
    "            print('Current Q-vals', self._Q_values[grid_hash])\n",
    "            print('Random sample ', random_sample, ' _epsilon ', self._epsilon, ' hence I chose ', \\\n",
    "                '*best*' if play_best_move else '*random*', ' move: ', chosen_move )\n",
    "            print('-----------------------------------')\n",
    "\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid, game_over):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_move, prev_grid = self._curr_move, self._curr_grid\n",
    "        self._curr_grid, self._curr_move = None, ''\n",
    "        new_grid_hash = self.hash_grid(new_grid)\n",
    "        prev_grid_hash = self.hash_grid(prev_grid)\n",
    "        reward = self._game_env.reward(self._player)\n",
    "        # Get max_a (Q(S', a))\n",
    "        possible_moves_s_dash = [] if game_over else self.get_empty_positions(new_grid)\n",
    "        self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "        max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "        max_q_value = self._Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "        if DEBUG:\n",
    "            print('*** UPDATING Q VALS ****')\n",
    "            game_over and print('*Game is over*')\n",
    "            print('Prev_grid: ', '\\n' + grid_to_string(prev_grid, False))\n",
    "            print('Prev_move: ', prev_move)\n",
    "            print('new_grid: ', '\\n' + grid_to_string(new_grid, False))\n",
    "            print('max_val_action: ', max_val_action)\n",
    "            print('Q-vals before: ', self._Q_values[prev_grid_hash])\n",
    "\n",
    "        # Update according to Q-learning formula\n",
    "        prev_q_val = self._Q_values[prev_grid_hash][prev_move]\n",
    "        self._Q_values[prev_grid_hash][prev_move] += self._learning_rate_alpha*(reward + self._discount_rate_gamma*max_q_value - prev_q_val)\n",
    "        if DEBUG:\n",
    "            print('reward: ', reward, 'max-q-val', max_q_value, 'discount', self._discount_rate_gamma,\\\n",
    "                'self._learning_rate_alpha',self._learning_rate_alpha)\n",
    "            print('Q-vals after: ', self._Q_values[prev_grid_hash])\n",
    "            print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games(_max_games_number, _our_player, _opponent_epsilon, _our_player_new_game_epsilon, _update_q_values, _progress_print=None):\n",
    "    _rewards = [None for _ in range(_max_games_number)]\n",
    "    _turns = ['X','O']\n",
    "    opponent =  OptimalPlayer(epsilon=_opponent_epsilon, player=_turns[0])\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent.player = _turns[0]\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != _our_player._player\n",
    "        _our_player.epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "        # epsilons[-1].append(_our_player.epsilon)\n",
    "\n",
    "        if (_progress_print and game % _progress_print == 0) or DEBUG:\n",
    "            print('Game ', game, ' begins.')\n",
    "            if DEBUG:\n",
    "                print('We play: ', _our_player.player)\n",
    "                input('awaiting input: ')\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.act(grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if _update_q_values and (opponent_turn and turn > 0 or end):\n",
    "                _our_player.update_q_values_(grid, game_over=end)\n",
    "            if end:\n",
    "                _rewards[game] = env.reward(_our_player._player)\n",
    "                break\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q1_epsilon = 0.1 # Chosen because they use this in Q2 so this will allow us to nicely compare\n",
    "\n",
    "max_games = 500\n",
    "q1_q_learnt_player = QLearntPlayer(env, _epsilon=q1_epsilon)\n",
    "print_Q_val_with_moves(q1_q_learnt_player._Q_values)\n",
    "avgs = []\n",
    "rewards = []\n",
    "total_wins = 0\n",
    "\n",
    "for game_epoch in range(max_games//250):\n",
    "    if game_epoch % 10 == 0:\n",
    "        print('Game ', game_epoch*250, ' begins.')\n",
    "    run_rewards = run_n_games(_max_games_number=250, _our_player=q1_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q1_epsilon, _update_q_values=True)\n",
    "    rewards+=run_rewards\n",
    "    avgs.append(np.average(run_rewards))\n",
    "    total_wins += sum(1 if rew ==1 else 0 for rew in run_rewards)\n",
    "\n",
    "print('Our agent won {} times'.format(total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs)\n",
    "plt.xticks(ticks=range(len(avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_qv = q1_q_learnt_player._Q_values\n",
    "print_Q_val_with_moves(_qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Q_val_with_moves(_qv, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Q_val_with_moves(_qv, only_vals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Plot average reward for every 250 games during training. Does decreasing epsilon help training\n",
    "compared to having a fixed epsilon? What is the effect of n∗?\n",
    "Expected answer: A figure showing average reward over time for different values of n∗ (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 500\n",
    "n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "rewards = {n_star: [] for n_star in n_stars}\n",
    "M_opt = {n_star: [] for n_star in n_stars}\n",
    "M_rand = {n_star: [] for n_star in n_stars}\n",
    "\n",
    "players = {}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "def calc_epsilon_factory(n_star, epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for n_star in n_stars:\n",
    "    # epsilons.append([])\n",
    "    q2_q_learnt_player = QLearntPlayer(env, _epsilon=max_epsilon)\n",
    "    players[n_star] = q2_q_learnt_player\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory(n_star=n_star, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        run_rewards = run_n_games(_max_games_number=epoch_size, _our_player=q2_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=calc_epsilon, _update_q_values=True)\n",
    "        rewards[n_star] += run_rewards\n",
    "\n",
    "        # Run 500 games for M_opt calculation\n",
    "        M_opt_rewards = run_n_games(_max_games_number=500, _our_player=q2_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_opt[n_star].append(np.average(M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games(_max_games_number=500, _our_player=q2_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_rand[n_star].append(np.average(M_rand_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players[1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = list(players.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "avgs = {n_star: [] for n_star in n_stars}\n",
    "for x in range(0,max_games, 250):\n",
    "    lower_index = x\n",
    "    upper_index = min(x+250, max_games-1)\n",
    "    for n_star in n_stars:\n",
    "        slice = rewards[n_star][lower_index:upper_index]\n",
    "        avgs[n_star].append(sum(slice)/len(slice))\n",
    "\n",
    "q2_data = pd.DataFrame(avgs)\n",
    "q2_data.index.name = 'epochs by 250'\n",
    "q2_data.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average rewards per 250 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the same plot as in exe 1 to ensure results are the same\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs[1.0])\n",
    "plt.xticks(ticks=range(len(avgs[1.0])), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs[1.0]))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n_star, rews) in rewards.items():\n",
    "    print('{} won {} games'.format(n_star, sum(1 if rew ==1 else 0 for rew in rews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards[1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "*Does decreasing epsilon help with training compared to fix epsilon?*\n",
    "\n",
    "(jl, April 22, 8:00pm)  **I cannot see a bug but it seems it virtually does not, which is very counterintuitive.** But the best value we seem to observe is when n_star = 1 but that is the same as not having any decrease at all and just hardcoding epsilon to 0.1. This is very strange indeed.\n",
    "\n",
    "\n",
    "*What is the effect of n\\*?*\n",
    "\n",
    "Based on our small sample of 4 n*, it seems the smaller n* the better: n*=1 gets better perf than n*=34.2 which in turn gets better than n*=1169 which is in turn much better than n*=40,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_df = pd.DataFrame(M_opt)\n",
    "M_opt_df.index.name = 'epochs by 250'\n",
    "M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_df = pd.DataFrame(M_rand)\n",
    "M_rand_df.index.name = 'epochs by 250'\n",
    "M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_df)\n",
    "g.set_ylabel('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "*Describe the differences and similarities between the curves and the one in the previous question*\n",
    "\n",
    "Just like in the previous question, we can see lower values of n* significantly outperforming higher values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "Choose the best value of $n^∗$ that you found in the previous section. Run Q-learning against Opt($\\epsilon_{opt}$) for\n",
    "different values of $\\epsilon_{opt}$ for 20’000 games – switch the 1st player after every game. Choose several values\n",
    "of $\\epsilon_{opt}$ from a reasonably wide interval between 0 to 1 – particularly, include $\\epsilon_{opt}$ = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents\n",
    "– for each value of $\\epsilon_{opt}$. Plot $M_{opt}$ and $M_{rand}$ over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon_{opt}$ (caption length\n",
    "< 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_star = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 500\n",
    "epoch_size = 250\n",
    "\n",
    "eps_opts = np.linspace(0,1,num=5)\n",
    "\n",
    "rewards_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "M_opt_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "M_rand_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "\n",
    "def calc_epsilon_factory_eps(epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/best_n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for eps_opt in eps_opts:\n",
    "    q4_q_learnt_player = QLearntPlayer(env, _epsilon=min_epsilon)\n",
    "    print('Current eps_opt = {}'.format(eps_opt))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory_eps(epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 4)\n",
    "        run_rewards = run_n_games(_max_games_number=epoch_size, _our_player=q4_q_learnt_player, _opponent_epsilon=eps_opt, \\\n",
    "            _our_player_new_game_epsilon=calc_epsilon, _update_q_values=True)\n",
    "        rewards_eps[eps_opt] += run_rewards\n",
    "\n",
    "        # Run 500 games for M_opt_eps calculation\n",
    "        M_opt_eps_rewards = run_n_games(_max_games_number=500, _our_player=q4_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_opt_eps[eps_opt].append(np.average(M_opt_eps_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_eps calculation\n",
    "        M_rand_eps_rewards = run_n_games(_max_games_number=500, _our_player=q4_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_rand_eps[eps_opt].append(np.average(M_rand_eps_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_eps_df = pd.DataFrame(M_opt_eps)\n",
    "M_opt_eps_df.index.name = 'epochs by 250'\n",
    "M_opt_eps_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_eps_df)\n",
    "g.set_ylabel('M_opt_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(jl, April 22, 8:00pm) **I think most likely there must be a bug** because it seems quite strange that the eps=0 option would learn to play the opponent perfectly right away after only 250 epochs. I can't remember anymore, but I don't think that is what we observed before? Definitely something to check. Also it seems weird to me that the process is not monotone. Would we not expect to only improve? How come we suddenly get worse? I guess there is some variance due to our epsilon (i.e. sometimes we explore and we don't choose the perfect option) so maybe that explains it?\n",
    "\n",
    "**EDIT (10pm)**: Maybe the reason why it underperforms against random player is that since its trained against a perfect player, it learns quickly how to almost perfectly avoid losses, but it never learns how to achieve wins since it never gets a chance to do that during training – hence it continues to play those positions randomly. **So perhaps this is not a bug afterall.** Still it does not make sense though why performance of 0.25 is bad against rand when it's so good here – 0.25 already gets exposure to winning positions so it should perform well? After 20k games I'd think it would have a chance to see winning positions often enough but perhaps I'm wrong.\n",
    "\n",
    "# Q5\n",
    "What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?\n",
    "\n",
    "The highest value of $M_{opt}$ is achieved by $\\epsilon=0.0$ and $\\epsilon=0.25$ and it is the value 0 which is the best that we can hope for against  $M_{opt}$. The highest value of $M_{rand}$ is near $0.8$ achieved by  $\\epsilon=0.75$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_opt_eps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_eps_df = pd.DataFrame(M_rand_eps)\n",
    "M_rand_eps_df.index.name = 'epochs by 250'\n",
    "M_rand_eps_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_eps_df)\n",
    "g.set_ylabel('M_rand_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again** it is super weird that the M_rand plot is not a mirror image of the M_opt; i.e. we'd expect that if 0.0 gets great performance again M_opt, it should get great performance here. This is not what seems to be happening though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 (answer)\n",
    "*Question 6. (Theory) Assume that Agent 1 learns by playing against $\\text{Opt}(0)$ and find the optimal Q-\n",
    "values $Q_1(s, a)$. In addition, assume that Agent 2 learns by playing against $\\text{Opt}(1)$ and find the optimal\n",
    "Q-values $Q_2(s, a)$. Do $Q_1(s, a)$ and $Q_2(s, a)$ have the same values? Justify your answer. (answer length\n",
    "< 150 words)*\n",
    "\n",
    "No, they will not have the same values. This is because if we play an optimal agent, we will never win and thus never observe a positive reward. Therefore all Q-values will be at best 0. However, playing against a random oponent, we will definitely win sometimes and as we will get closer and closer to convergence, we will play better and better and win more and more. We will therefore definitely observe positive rewards at times. Hence, the Q-values will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Learning by practice\n",
    "In this section, your are supposed to ask whether Q-learning can learn to play Tic Tac Toe by only\n",
    "playing against itself. For different values of $\\epsilon \\in [0, 1)$, run a Q-learning agent against itself for 20’000\n",
    "games – i.e. both players use the same set of Q-values and update the same set of Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (code below)\n",
    "*Question 7. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_self(_max_games_number, _our_player, _our_player_new_game_epsilon, _update_q_values, _progress_print=None):\n",
    "    _rewards = {'our_player': [None for _ in range(_max_games_number)],'opponent': [None for _ in range(_max_games_number)]}\n",
    "    _turns = ['X','O']\n",
    "    opponent = QLearntPlayer(_our_player._game_env, _our_player._epsilon,_our_player._discount_rate_gamma, _our_player._learning_rate_alpha)\n",
    "\n",
    "    # Ensure they share Q_values... I think this should work but it seems (as of April 22, 8pm)\n",
    "    # that there is a bug: see more below\n",
    "    opponent._Q_values = _our_player._Q_values\n",
    "\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        if _progress_print and game % _progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent = opponent.prepare_new_game_(_turns[0])\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent._player != _our_player._player\n",
    "        _our_player.epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "        opponent.epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent._player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.choose_move_(grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if end:\n",
    "                _update_q_values and _our_player.update_q_values_(grid, game_over = end)\n",
    "                _update_q_values and opponent.update_q_values_(grid, game_over = end)\n",
    "                _rewards['our_player'][game] = env.reward(_our_player._player)\n",
    "                _rewards['opponent'][game] = env.reward(opponent._player)\n",
    "                break\n",
    "            else:\n",
    "                if opponent_turn:\n",
    "                    turn!=0 and _update_q_values and _our_player.update_q_values_(grid, game_over = end)\n",
    "                else:\n",
    "                    turn!=0 and _update_q_values and opponent.update_q_values_(grid, game_over = end)\n",
    "\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 500\n",
    "epoch_size = 250\n",
    "\n",
    "eps_selfs = np.linspace(0,0.99,num=5)\n",
    "\n",
    "M_opt_self = {eps_opt: [] for eps_opt in eps_selfs}\n",
    "M_rand_self = {eps_opt: [] for eps_opt in eps_selfs}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "for eps_s in eps_selfs:\n",
    "    q7_q_learnt_player = QLearntPlayer(env, _epsilon=min_epsilon)\n",
    "    print('Current eps_s = {}'.format(eps_s))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = lambda game_number_n: eps_s\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 7)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=epoch_size, _our_player=q7_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=calc_epsilon, _update_q_values=True)\n",
    "\n",
    "        # Run 500 games for M_opt_self calculation\n",
    "        M_opt_self_rewards = run_n_games(_max_games_number=500, _our_player=q7_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_opt_self[eps_s].append(np.average(M_opt_self_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_self calculation\n",
    "        M_rand_self_rewards = run_n_games(_max_games_number=500, _our_player=q7_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_rand_self[eps_s].append(np.average(M_rand_self_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_df = pd.DataFrame(M_opt_self)\n",
    "M_opt_self_df.index.name = 'epochs by 250'\n",
    "M_opt_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_df)\n",
    "g.set_ylabel('M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_df = pd.DataFrame(M_opt_self[eps_selfs[1]])\n",
    "M_opt_self_df.index.name = 'epochs by 250'\n",
    "M_opt_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_df)\n",
    "g.set_ylabel('M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_self_df = pd.DataFrame(M_rand_self)\n",
    "M_rand_self_df.index.name = 'epochs by 250'\n",
    "M_rand_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_self_df)\n",
    "g.set_ylabel('M_rand_self')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (answer)\n",
    "*Question 7. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*\n",
    "\n",
    "(jl, April 22 8:00 pm) **It seems that there is some bug here** because very little learning seems to be going on. I would expect the agent here to learn at least as well as when playing a random oponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (code below)\n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^∗$ – instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^∗$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^∗$ (caption\n",
    "length < 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 500\n",
    "n_stars =  np.geomspace(1, 40000, num=4) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "rewards_self_n_stars = {n_star: {'our_player': [], 'opponent':[]} for n_star in n_stars}\n",
    "M_opt_self_n_stars = {n_star: [] for n_star in n_stars}\n",
    "M_rand_self_n_stars = {n_star: [] for n_star in n_stars}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "def calc_epsilon_factory(n_star, epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for n_star in n_stars:\n",
    "    q8_q_learnt_player = QLearntPlayer(env, _epsilon=max_epsilon)\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory(n_star=n_star, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 8)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=epoch_size, _our_player=q8_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=calc_epsilon, _update_q_values=True)\n",
    "        rewards_self_n_stars[n_star]['our_player'] += run_rewards['our_player']\n",
    "        rewards_self_n_stars[n_star]['opponent'] += run_rewards['opponent']\n",
    "\n",
    "        # Run 500 games for M_opt_self_n_stars calculation\n",
    "        M_opt_self_n_stars_rewards = run_n_games(_max_games_number=500, _our_player=q8_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_opt_self_n_stars[n_star].append(np.average(M_opt_self_n_stars_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_self_n_stars calculation\n",
    "        M_rand_self_n_stars_rewards = run_n_games(_max_games_number=500, _our_player=q8_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        M_rand_self_n_stars[n_star].append(np.average(M_rand_self_n_stars_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_self_n_stars_df = pd.DataFrame(M_rand_self_n_stars)\n",
    "M_rand_self_n_stars_df.index.name = 'epochs by 250'\n",
    "M_rand_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_self_n_stars_df)\n",
    "g.set_ylabel('M_rand_self_n_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_n_stars_df = pd.DataFrame(M_opt_self_n_stars)\n",
    "M_opt_self_n_stars_df.index.name = 'epochs by 250'\n",
    "M_opt_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_n_stars_df)\n",
    "g.set_ylabel('M_opt_self_n_stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (answer)\n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^∗$ – instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^∗$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^∗$ (caption\n",
    "length < 100 words).\n",
    "\n",
    "(jl, April 22 8:00 pm) **Just like in Q7, I think this is buggy** because very little learning seems to be going on. I would expect the agent here to learn at least as well as when playing a random oponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "*Question 9. What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?*\n",
    "\n",
    "**TODO: Answer once sure the above code is not buggy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n",
    "*Question 10. For three board arrangements (i.e. states s), visualize Q-values of available actions (e.g.\n",
    "using heat maps). Does the result make sense? Did the agent learn the game well?\n",
    "Expected answer: A figure with 3 subplots of 3 different states with Q-values shown at available actions\n",
    "(caption length < 200 words).*\n",
    "\n",
    "**TODO: Answer once sure the above code is not buggy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep Q-Learning\n",
    "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch\n",
    "again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of\n",
    "Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is\n",
    "to approximate Q-values by a neural network instead of a look-up table as in Tabular Q-learning. For\n",
    "implementation, you can use ideas from the DQN tutorials of Keras and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Learning from experts\n",
    "Implement the DQN algorithm. To check the algorithm, run a DQN agent with a fixed and arbitrary\n",
    "$\\epsilon \\in [0,1)$ against Opt(0.5) for 20’000 games – switch the 1st player after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "*Plot average reward and average training loss for every 250 games during training. Does\n",
    "the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure with two subplots (caption length $<$ 50 words). Specify your choice of $\\epsilon$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We use $\\epsilon = 0.05$ as that is the value that Mnih at el (2015) use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def state_to_torch(game_state: np.ndarray):\n",
    "    return torch.cat((torch.from_numpy((game_state == 1).astype(int)).view(3,3,1), torch.from_numpy((game_state == -1).astype(int)).view(3,3,1)), dim=2).to(device)\n",
    "\n",
    "def game_state_converts_to_pytorch_correctly():\n",
    "    game_state = np.array([ [ 1.,  1.,  1.], \\\n",
    "                            [ 1.,  0., -1.], \\\n",
    "                            [-1., -1.,  1.]])\n",
    "    torch_repre = state_to_torch(game_state)\n",
    "    expected_our_positions = torch.from_numpy((game_state==1).astype(int))\n",
    "    torch_repre_our_positions = torch_repre[:,:,0]\n",
    "    assert (expected_our_positions == torch_repre_our_positions).all()\n",
    "    expected_opponent_positions = torch.from_numpy((game_state==-1).astype(int))\n",
    "    torch_repre_opponent_positions = torch_repre[:,:,1]\n",
    "    assert (expected_opponent_positions == torch_repre_opponent_positions).all()\n",
    "    return True\n",
    "\n",
    "assert game_state_converts_to_pytorch_correctly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell's code is taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html?highlight=huber\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net =  nn.Sequential(\n",
    "            nn.LazyLinear(128),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(128),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(9))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        input_in_1D = inp.view(inp.size(0), -1)\n",
    "        return self.net(input_in_1D)\n",
    "\n",
    "summary(DQNet().to(device), (3,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Player(BasePlayer):\n",
    "    def __init__(self):\n",
    "        self.model = DQNet()\n",
    "        LEARNING_RATE = 5 * 10e-4 # Given in the instructions PDF\n",
    "        DISCOUNT_RATE_GAMMA = 0.99\n",
    "        BATCH_SIZE = 64\n",
    "        BUFFER_SIZE = 10_000\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.SmoothL1Loss() # \"When delta is set to 1, this loss is equivalent to SmoothL1Loss.\" (PyTorch HuberLoss documentation)\n",
    "\n",
    "    def choose_move(self, grid, epsilon):\n",
    "        possible_moves\n",
    "        if random.random() > \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x == 1).astype(int), (x == -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat((torch.from_numpy((x == 1).astype(int)).view(3,3,1), torch.from_numpy((x == -1).astype(int)).view(3,3,1)), dim=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,:,0], y[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y[:,:,0] == torch.from_numpy((x==1).astype(int))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f371659e15671c4bf19c781ff73734bc385c323133a3685f5b4f679a2745d0"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
