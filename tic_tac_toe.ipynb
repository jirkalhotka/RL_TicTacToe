{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against\n",
    "Opt(eps_opt) for some eps_opt âˆˆ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm,\n",
    "run a Q-learning agent, with a fixed and arbitrary eps âˆˆ [0, 1), against Opt(0.5) for 20â€™000 games â€“ switch\n",
    "the 1st player after every game.\n",
    "Question 1. Plot average reward for every 250 games during training â€“ i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearntPlayer:\n",
    "    def __init__(self, game_env: TictactoeEnv, epsilon: float, discount_rate_gamma = 0.99, learning_rate_alpha = 0.5):\n",
    "        self.game_env = game_env\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_rate_gamma = discount_rate_gamma\n",
    "        self.learning_rate_alpha = learning_rate_alpha\n",
    "        self.Q_values = {Turns[0]: {}, Turns[1]: {}}\n",
    "        self.prev_move = None\n",
    "        self.prev_grid = None\n",
    "        self.player = None # 'X' or 'O'\n",
    "        self.player_Q_values = None\n",
    "\n",
    "    def get_empty_positions(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "\n",
    "    def hash_grid(self, grid: np.ndarray):\n",
    "        return grid.tobytes()\n",
    "\n",
    "    def prepare_new_game_(self, player):\n",
    "        self.prev_move = None\n",
    "        self.prev_grid = None\n",
    "        self.curr_grid = None\n",
    "        self.curr_move = None\n",
    "        assert player == 'X' or player == 'O'\n",
    "        self.player = player\n",
    "        self.player_Q_values = self.Q_values[player]\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, possible_moves, grid_hash):\n",
    "        if len(possible_moves) == 0:\n",
    "            q_val = self.player_Q_values[grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        return max(possible_moves, key=self.player_Q_values[grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, grid_hash, possible_moves):\n",
    "        if grid_hash not in self.player_Q_values:\n",
    "            self.player_Q_values[grid_hash] = {} if len(possible_moves) > 0 else {'': 0}\n",
    "        for mv in possible_moves:\n",
    "            if mv not in self.player_Q_values[grid_hash]: self.player_Q_values[grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, grid):\n",
    "        grid_hash = self.hash_grid(grid)\n",
    "        # Get moves\n",
    "        possible_moves = self.get_empty_positions(grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        if random.random() >= self.epsilon:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self.curr_grid = grid\n",
    "        self.curr_move = chosen_move\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_grid, prev_move = self.prev_grid, self.prev_move\n",
    "        self.prev_grid = self.curr_grid\n",
    "        self.prev_move = self.curr_move\n",
    "        self.curr_grid, self.curr_move = None, None\n",
    "        if prev_grid is not None and prev_move is not None:\n",
    "            new_grid_hash = self.hash_grid(new_grid)\n",
    "            prev_grid_hash = self.hash_grid(prev_grid)\n",
    "            reward = self.game_env.reward(self.player)\n",
    "            # Get max_a (Q(S', a))\n",
    "            possible_moves_s_dash = self.get_empty_positions(new_grid)\n",
    "            self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "            max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "            max_q_value = self.player_Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "            # Update according to Q-learning formula\n",
    "            prev_q_val = self.player_Q_values[prev_grid_hash][prev_move]\n",
    "            self.player_Q_values[prev_grid_hash][prev_move] += self.learning_rate_alpha*(reward + self.discount_rate_gamma*max_q_value - prev_q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games(max_games_number, our_player, opponent_epsilon, our_player_new_game_epsilon, update_q_values, progress_print=None):\n",
    "    _rewards = [None for _ in range(max_games_number)]\n",
    "    _turns = ['X','O']\n",
    "    opponent =  OptimalPlayer(epsilon=opponent_epsilon, player=_turns[0])\n",
    "\n",
    "    for game in range(max_games_number):\n",
    "        if progress_print and game % progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent.player = _turns[0]\n",
    "        our_player = q_learnt_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != our_player.player\n",
    "        our_player.epsilon = our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.act(grid)\n",
    "            else:\n",
    "                chosen_move = our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if opponent_turn:\n",
    "                update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "            if end:\n",
    "                update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "                _rewards[game] = env.reward(our_player.player)\n",
    "                break\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_games = 20000\n",
    "q_learnt_player = QLearntPlayer(env, epsilon=0.2)\n",
    "avgs = []\n",
    "rewards = []\n",
    "total_wins = 0\n",
    "\n",
    "for game_epoch in range(max_games//250):\n",
    "    if game_epoch % 10 == 0:\n",
    "        print('Game ', game_epoch*250, ' begins.')\n",
    "    run_rewards = run_n_games(max_games_number=250, our_player=q_learnt_player, opponent_epsilon=0.5, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0.2, update_q_values=True)\n",
    "    rewards+=run_rewards\n",
    "    avgs.append(np.average(run_rewards))\n",
    "    total_wins += sum(1 if rew ==1 else 0 for rew in run_rewards)\n",
    "\n",
    "print('Our agent won {} times'.format(total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs)\n",
    "plt.xticks(ticks=range(len(avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Plot average reward for every 250 games during training. Does decreasing epsilon help training\n",
    "compared to having a fixed epsilon? What is the effect of nâˆ—?\n",
    "Expected answer: A figure showing average reward over time for different values of nâˆ— (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "n_stars =  np.geomspace(1, 40000, num=4) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "rewards = {n_star: [] for n_star in n_stars}\n",
    "M_opt = {n_star: [] for n_star in n_stars}\n",
    "M_rand = {n_star: [] for n_star in n_stars}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "def calc_epsilon_factory(n_star, epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for n_star in n_stars:\n",
    "    q_learnt_player = QLearntPlayer(env, epsilon=max_epsilon)\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory(n_star=n_star, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        run_rewards = run_n_games(max_games_number=epoch_size, our_player=q_learnt_player, opponent_epsilon=0.5, \\\n",
    "            our_player_new_game_epsilon=calc_epsilon, update_q_values=True)\n",
    "        rewards[n_star] += run_rewards\n",
    "\n",
    "        # Run 500 games for M_opt calculation\n",
    "        M_opt_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=0, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_opt[n_star].append(np.average(M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=1, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_rand[n_star].append(np.average(M_rand_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "avgs = {n_star: [] for n_star in n_stars}\n",
    "for x in range(0,max_games, 250):\n",
    "    lower_index = x\n",
    "    upper_index = min(x+250, max_games-1)\n",
    "    for n_star in n_stars:\n",
    "        slice = rewards[n_star][lower_index:upper_index]\n",
    "        avgs[n_star].append(sum(slice)/len(slice))\n",
    "\n",
    "q2_data = pd.DataFrame(avgs)\n",
    "q2_data.index.name = 'epochs by 250'\n",
    "q2_data.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average rewards per 250 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the same plot as in exe 1 to ensure results are the same\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs[1.0])\n",
    "plt.xticks(ticks=range(len(avgs[1.0])), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs[1.0]))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n_star, rews) in rewards.items():\n",
    "    print('{} won {} games'.format(n_star, sum(1 if rew ==1 else 0 for rew in rews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards[1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "*Does decreasing epsilon help with training compared to fix epsilon?*\n",
    "\n",
    " The data seems to show it does not but one would expect it to work?\n",
    "\n",
    "\n",
    "*What is the effect of n\\*?*\n",
    "\n",
    "Based on our small sample of 4 n*, it seems the smaller n* the better: n*=1 gets better perf than n*=34.2 which in turn gets better than n*=1169 which is in turn much better than n*=40,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_df = pd.DataFrame(M_opt)\n",
    "M_opt_df.index.name = 'epochs by 250'\n",
    "M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_df = pd.DataFrame(M_rand)\n",
    "M_rand_df.index.name = 'epochs by 250'\n",
    "M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_df)\n",
    "g.set_ylabel('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "*Describe the differences and similarities between the curves and the one in the previous question*\n",
    "\n",
    "Just like in the previous question, we can see lower values of n* significantly outperforming higher values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "Choose the best value of $n^âˆ—$ that you found in the previous section. Run Q-learning against Opt($\\epsilon_{opt}$) for\n",
    "different values of $\\epsilon_{opt}$ for 20â€™000 games â€“ switch the 1st player after every game. Choose several values\n",
    "of $\\epsilon_{opt}$ from a reasonably wide interval between 0 to 1 â€“ particularly, include $\\epsilon_{opt}$ = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for your agents\n",
    "â€“ for each value of $\\epsilon_{opt}$. Plot $M_{opt}$ and $M_{rand}$ over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon_{opt}$ (caption length\n",
    "< 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_star = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f371659e15671c4bf19c781ff73734bc385c323133a3685f5b4f679a2745d0"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
