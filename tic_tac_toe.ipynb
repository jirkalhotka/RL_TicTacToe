{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against\n",
    "Opt(eps_opt) for some eps_opt âˆˆ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm,\n",
    "run a Q-learning agent, with a fixed and arbitrary eps âˆˆ [0, 1), against Opt(0.5) for 20â€™000 games â€“ switch\n",
    "the 1st player after every game.\n",
    "Question 1. Plot average reward for every 250 games during training â€“ i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearntPlayer:\n",
    "    def __init__(self, game_env: TictactoeEnv, epsilon: float, discount_rate_gamma = 0.99, learning_rate_alpha = 0.5):\n",
    "        self.game_env = game_env\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_rate_gamma = discount_rate_gamma\n",
    "        self.learning_rate_alpha = learning_rate_alpha\n",
    "        self.Q_values = {Turns[0]: {}, Turns[1]: {}}\n",
    "        self.prev_move = None\n",
    "        self.prev_grid = None\n",
    "        self.player = None # 'X' or 'O'\n",
    "        self.player_Q_values = None\n",
    "\n",
    "    def get_empty_positions(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "\n",
    "    def hash_grid(self, grid: np.ndarray):\n",
    "        return grid.tobytes()\n",
    "\n",
    "    def prepare_new_game_(self, player):\n",
    "        self.prev_move = None\n",
    "        self.prev_grid = None\n",
    "        self.curr_grid = None\n",
    "        self.curr_move = None\n",
    "        assert player == 'X' or player == 'O'\n",
    "        self.player = player\n",
    "        self.player_Q_values = self.Q_values[player]\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, possible_moves, grid_hash):\n",
    "        if len(possible_moves) == 0:\n",
    "            q_val = self.player_Q_values[grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        return max(possible_moves, key=self.player_Q_values[grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, grid_hash, possible_moves):\n",
    "        if grid_hash not in self.player_Q_values:\n",
    "            self.player_Q_values[grid_hash] = {} if len(possible_moves) > 0 else {'': 0}\n",
    "        for mv in possible_moves:\n",
    "            if mv not in self.player_Q_values[grid_hash]: self.player_Q_values[grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, grid):\n",
    "        grid_hash = self.hash_grid(grid)\n",
    "        # Get moves\n",
    "        possible_moves = self.get_empty_positions(grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        if random.random() >= self.epsilon:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self.curr_grid = grid\n",
    "        self.curr_move = chosen_move\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_grid, prev_move = self.prev_grid, self.prev_move\n",
    "        self.prev_grid = self.curr_grid\n",
    "        self.prev_move = self.curr_move\n",
    "        self.curr_grid, self.curr_move = None, None\n",
    "        if prev_grid is not None and prev_move is not None:\n",
    "            new_grid_hash = self.hash_grid(new_grid)\n",
    "            prev_grid_hash = self.hash_grid(prev_grid)\n",
    "            reward = self.game_env.reward(self.player)\n",
    "            # Get max_a (Q(S', a))\n",
    "            possible_moves_s_dash = self.get_empty_positions(new_grid)\n",
    "            self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "            max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "            max_q_value = self.player_Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "            # Update according to Q-learning formula\n",
    "            prev_q_val = self.player_Q_values[prev_grid_hash][prev_move]\n",
    "            self.player_Q_values[prev_grid_hash][prev_move] += self.learning_rate_alpha*(reward + self.discount_rate_gamma*max_q_value - prev_q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games(max_games_number, our_player, opponent_epsilon, our_player_new_game_epsilon, update_q_values, progress_print=None):\n",
    "    _rewards = [None for _ in range(max_games_number)]\n",
    "    _turns = ['X','O']\n",
    "    opponent =  OptimalPlayer(epsilon=opponent_epsilon, player=_turns[0])\n",
    "\n",
    "    for game in range(max_games_number):\n",
    "        if progress_print and game % progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent.player = _turns[0]\n",
    "        our_player = q_learnt_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != our_player.player\n",
    "        our_player.epsilon = our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.act(grid)\n",
    "            else:\n",
    "                chosen_move = our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if opponent_turn:\n",
    "                update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "            if end:\n",
    "                update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "                _rewards[game] = env.reward(our_player.player)\n",
    "                break\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q1_epsilon = 0.1 # Chosen because they use this in Q2 so this will allow us to nicely compare\n",
    "\n",
    "max_games = 20000\n",
    "q_learnt_player = QLearntPlayer(env, epsilon=q1_epsilon)\n",
    "avgs = []\n",
    "rewards = []\n",
    "total_wins = 0\n",
    "\n",
    "for game_epoch in range(max_games//250):\n",
    "    if game_epoch % 10 == 0:\n",
    "        print('Game ', game_epoch*250, ' begins.')\n",
    "    run_rewards = run_n_games(max_games_number=250, our_player=q_learnt_player, opponent_epsilon=0.5, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: q1_epsilon, update_q_values=True)\n",
    "    rewards+=run_rewards\n",
    "    avgs.append(np.average(run_rewards))\n",
    "    total_wins += sum(1 if rew ==1 else 0 for rew in run_rewards)\n",
    "\n",
    "print('Our agent won {} times'.format(total_wins))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs)\n",
    "plt.xticks(ticks=range(len(avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Plot average reward for every 250 games during training. Does decreasing epsilon help training\n",
    "compared to having a fixed epsilon? What is the effect of nâˆ—?\n",
    "Expected answer: A figure showing average reward over time for different values of nâˆ— (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "rewards = {n_star: [] for n_star in n_stars}\n",
    "M_opt = {n_star: [] for n_star in n_stars}\n",
    "M_rand = {n_star: [] for n_star in n_stars}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "def calc_epsilon_factory(n_star, epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for n_star in n_stars:\n",
    "    q_learnt_player = QLearntPlayer(env, epsilon=max_epsilon)\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory(n_star=n_star, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        run_rewards = run_n_games(max_games_number=epoch_size, our_player=q_learnt_player, opponent_epsilon=0.5, \\\n",
    "            our_player_new_game_epsilon=calc_epsilon, update_q_values=True)\n",
    "        rewards[n_star] += run_rewards\n",
    "\n",
    "        # Run 500 games for M_opt calculation\n",
    "        M_opt_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=0, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_opt[n_star].append(np.average(M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=1, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_rand[n_star].append(np.average(M_rand_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "avgs = {n_star: [] for n_star in n_stars}\n",
    "for x in range(0,max_games, 250):\n",
    "    lower_index = x\n",
    "    upper_index = min(x+250, max_games-1)\n",
    "    for n_star in n_stars:\n",
    "        slice = rewards[n_star][lower_index:upper_index]\n",
    "        avgs[n_star].append(sum(slice)/len(slice))\n",
    "\n",
    "q2_data = pd.DataFrame(avgs)\n",
    "q2_data.index.name = 'epochs by 250'\n",
    "q2_data.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average rewards per 250 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the same plot as in exe 1 to ensure results are the same\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs[1.0])\n",
    "plt.xticks(ticks=range(len(avgs[1.0])), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs[1.0]))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n_star, rews) in rewards.items():\n",
    "    print('{} won {} games'.format(n_star, sum(1 if rew ==1 else 0 for rew in rews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards[1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "*Does decreasing epsilon help with training compared to fix epsilon?*\n",
    "\n",
    "(jl, April 22, 8:00pm)  **I cannot see a bug but it seems it virtually does not, which is very counterintuitive.** But the best value we seem to observe is when n_star = 1 but that is the same as not having any decrease at all and just hardcoding epsilon to 0.1. This is very strange indeed.\n",
    "\n",
    "\n",
    "*What is the effect of n\\*?*\n",
    "\n",
    "Based on our small sample of 4 n*, it seems the smaller n* the better: n*=1 gets better perf than n*=34.2 which in turn gets better than n*=1169 which is in turn much better than n*=40,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_df = pd.DataFrame(M_opt)\n",
    "M_opt_df.index.name = 'epochs by 250'\n",
    "M_opt_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_df)\n",
    "g.set_ylabel('M_opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_df = pd.DataFrame(M_rand)\n",
    "M_rand_df.index.name = 'epochs by 250'\n",
    "M_rand_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_df)\n",
    "g.set_ylabel('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3\n",
    "*Describe the differences and similarities between the curves and the one in the previous question*\n",
    "\n",
    "Just like in the previous question, we can see lower values of n* significantly outperforming higher values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "Choose the best value of $n^âˆ—$ that you found in the previous section. Run Q-learning against Opt($\\epsilon_{opt}$) for\n",
    "different values of $\\epsilon_{opt}$ for 20â€™000 games â€“ switch the 1st player after every game. Choose several values\n",
    "of $\\epsilon_{opt}$ from a reasonably wide interval between 0 to 1 â€“ particularly, include $\\epsilon_{opt}$ = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for your agents\n",
    "â€“ for each value of $\\epsilon_{opt}$. Plot $M_{opt}$ and $M_{rand}$ over time. What do you observe? How can you explain it?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon_{opt}$ (caption length\n",
    "< 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_star = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "epoch_size = 250\n",
    "\n",
    "eps_opts = np.linspace(0,1,num=5)\n",
    "\n",
    "rewards_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "M_opt_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "M_rand_eps = {eps_opt: [] for eps_opt in eps_opts}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "\n",
    "def calc_epsilon_factory_eps(epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/best_n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for eps_opt in eps_opts:\n",
    "    q_learnt_player = QLearntPlayer(env, epsilon=min_epsilon)\n",
    "    print('Current eps_opt = {}'.format(eps_opt))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory_eps(epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 4)\n",
    "        run_rewards = run_n_games(max_games_number=epoch_size, our_player=q_learnt_player, opponent_epsilon=eps_opt, \\\n",
    "            our_player_new_game_epsilon=calc_epsilon, update_q_values=True)\n",
    "        rewards_eps[eps_opt] += run_rewards\n",
    "\n",
    "        # Run 500 games for M_opt_eps calculation\n",
    "        M_opt_eps_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=0, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_opt_eps[eps_opt].append(np.average(M_opt_eps_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_eps calculation\n",
    "        M_rand_eps_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=1, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_rand_eps[eps_opt].append(np.average(M_rand_eps_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_eps_df = pd.DataFrame(M_opt_eps)\n",
    "M_opt_eps_df.index.name = 'epochs by 250'\n",
    "M_opt_eps_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_eps_df)\n",
    "g.set_ylabel('M_opt_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(jl, April 22, 8:00pm) **I think most likely there must be a bug** because it seems quite strange that the eps=0 option would learn to play the opponent perfectly right away after only 250 epochs. I can't remember anymore, but I don't think that is what we observed before? Definitely something to check. Also it seems weird to me that the process is not monotone. Would we not expect to only improve? How come we suddenly get worse? I guess there is some variance due to our epsilon (i.e. sometimes we explore and we don't choose the perfect option) so maybe that explains it?\n",
    "\n",
    "**EDIT (10pm)**: Maybe the reason why it underperforms against random player is that since its trained against a perfect player, it learns quickly how to almost perfectly avoid losses, but it never learns how to achieve wins since it never gets a chance to do that during training â€“ hence it continues to play those positions randomly. **So perhaps this is not a bug afterall.** Still it does not make sense though why performance of 0.25 is bad against rand when it's so good here â€“ 0.25 already gets exposure to winning positions so it should perform well? After 20k games I'd think it would have a chance to see winning positions often enough but perhaps I'm wrong.\n",
    "\n",
    "# Q5\n",
    "What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20â€™000 games?\n",
    "\n",
    "The highest value of $M_{opt}$ is achieved by $\\epsilon=0.0$ and $\\epsilon=0.25$ and it is the value 0 which is the best that we can hope for against  $M_{opt}$. The highest value of $M_{rand}$ is near $0.8$ achieved by  $\\epsilon=0.75$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_opt_eps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_eps_df = pd.DataFrame(M_rand_eps)\n",
    "M_rand_eps_df.index.name = 'epochs by 250'\n",
    "M_rand_eps_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_eps_df)\n",
    "g.set_ylabel('M_rand_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again** it is super weird that the M_rand plot is not a mirror image of the M_opt; i.e. we'd expect that if 0.0 gets great performance again M_opt, it should get great performance here. This is not what seems to be happening though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 (answer)\n",
    "*Question 6. (Theory) Assume that Agent 1 learns by playing against $\\text{Opt}(0)$ and find the optimal Q-\n",
    "values $Q_1(s, a)$. In addition, assume that Agent 2 learns by playing against $\\text{Opt}(1)$ and find the optimal\n",
    "Q-values $Q_2(s, a)$. Do $Q_1(s, a)$ and $Q_2(s, a)$ have the same values? Justify your answer. (answer length\n",
    "< 150 words)*\n",
    "\n",
    "No, they will not have the same values. This is because if we play an optimal agent, we will never win and thus never observe a positive reward. Therefore all Q-values will be at best 0. However, playing against a random oponent, we will definitely win sometimes and as we will get closer and closer to convergence, we will play better and better and win more and more. We will therefore definitely observe positive rewards at times. Hence, the Q-values will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Learning by practice\n",
    "In this section, your are supposed to ask whether Q-learning can learn to play Tic Tac Toe by only\n",
    "playing against itself. For different values of $\\epsilon \\in [0, 1)$, run a Q-learning agent against itself for 20â€™000\n",
    "games â€“ i.e. both players use the same set of Q-values and update the same set of Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (code below)\n",
    "*Question 7. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_self(max_games_number, our_player, our_player_new_game_epsilon, update_q_values, progress_print=None):\n",
    "    _rewards = {'our_player': [None for _ in range(max_games_number)],'opponent': [None for _ in range(max_games_number)]}\n",
    "    _turns = ['X','O']\n",
    "    opponent = QLearntPlayer(our_player.game_env, our_player.epsilon,our_player.discount_rate_gamma, our_player.learning_rate_alpha)\n",
    "\n",
    "    # Ensure they share Q_values... I think this should work but it seems (as of April 22, 8pm)\n",
    "    # that there is a bug: see more below\n",
    "    opponent.Q_values = our_player.Q_values\n",
    "\n",
    "\n",
    "    for game in range(max_games_number):\n",
    "        if progress_print and game % progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent = opponent.prepare_new_game_(_turns[0])\n",
    "        our_player = q_learnt_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != our_player.player\n",
    "        our_player.epsilon = our_player_new_game_epsilon(game_number_n=game)\n",
    "        opponent.epsilon = our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.choose_move_(grid)\n",
    "            else:\n",
    "                chosen_move = our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if end:\n",
    "                update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "                update_q_values and opponent.update_q_values_(grid)\n",
    "                _rewards['our_player'][game] = env.reward(our_player.player)\n",
    "                _rewards['opponent'][game] = env.reward(opponent.player)\n",
    "                break\n",
    "            else:\n",
    "                if opponent_turn:\n",
    "                    update_q_values and q_learnt_player.update_q_values_(grid)\n",
    "                else:\n",
    "                    update_q_values and opponent.update_q_values_(grid)\n",
    "\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "epoch_size = 250\n",
    "\n",
    "eps_selfs = np.linspace(0,0.99,num=5)\n",
    "\n",
    "M_opt_self = {eps_opt: [] for eps_opt in eps_selfs}\n",
    "M_rand_self = {eps_opt: [] for eps_opt in eps_selfs}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "for eps_s in eps_selfs:\n",
    "    q_learnt_player = QLearntPlayer(env, epsilon=min_epsilon)\n",
    "    print('Current eps_s = {}'.format(eps_s))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = lambda game_number_n: eps_s\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 7)\n",
    "        run_rewards = run_n_games_against_self(max_games_number=epoch_size, our_player=q_learnt_player, \\\n",
    "            our_player_new_game_epsilon=calc_epsilon, update_q_values=True)\n",
    "\n",
    "        # Run 500 games for M_opt_self calculation\n",
    "        M_opt_self_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=0, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_opt_self[eps_s].append(np.average(M_opt_self_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_self calculation\n",
    "        M_rand_self_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=1, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_rand_self[eps_s].append(np.average(M_rand_self_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_df = pd.DataFrame(M_opt_self)\n",
    "M_opt_self_df.index.name = 'epochs by 250'\n",
    "M_opt_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_df)\n",
    "g.set_ylabel('M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_df = pd.DataFrame(M_opt_self[eps_selfs[1]])\n",
    "M_opt_self_df.index.name = 'epochs by 250'\n",
    "M_opt_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_df)\n",
    "g.set_ylabel('M_opt_self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_self_df = pd.DataFrame(M_rand_self)\n",
    "M_rand_self_df.index.name = 'epochs by 250'\n",
    "M_rand_self_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_self_df)\n",
    "g.set_ylabel('M_rand_self')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 (answer)\n",
    "*Question 7. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words).*\n",
    "\n",
    "(jl, April 22 8:00 pm) **It seems that there is some bug here** because very little learning seems to be going on. I would expect the agent here to learn at least as well as when playing a random oponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (code below)\n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^âˆ—$ â€“ instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^âˆ—$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^âˆ—$ (caption\n",
    "length < 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "n_stars =  np.geomspace(1, 40000, num=4) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "rewards_self_n_stars = {n_star: {'our_player': [], 'opponent':[]} for n_star in n_stars}\n",
    "M_opt_self_n_stars = {n_star: [] for n_star in n_stars}\n",
    "M_rand_self_n_stars = {n_star: [] for n_star in n_stars}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "def calc_epsilon_factory(n_star, epoch_size, game_epoch):\n",
    "        def calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(min_epsilon, max_epsilon*(1-(real_game_number/n_star)))\n",
    "        return calc_epsilon\n",
    "\n",
    "for n_star in n_stars:\n",
    "    q_learnt_player = QLearntPlayer(env, epsilon=max_epsilon)\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "\n",
    "    for game_epoch in range(max_games//epoch_size):\n",
    "        calc_epsilon = calc_epsilon_factory(n_star=n_star, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 8)\n",
    "        run_rewards = run_n_games_against_self(max_games_number=epoch_size, our_player=q_learnt_player, \\\n",
    "            our_player_new_game_epsilon=calc_epsilon, update_q_values=True)\n",
    "        rewards_self_n_stars[n_star]['our_player'] += run_rewards['our_player']\n",
    "        rewards_self_n_stars[n_star]['opponent'] += run_rewards['opponent']\n",
    "\n",
    "        # Run 500 games for M_opt_self_n_stars calculation\n",
    "        M_opt_self_n_stars_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=0, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_opt_self_n_stars[n_star].append(np.average(M_opt_self_n_stars_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand_self_n_stars calculation\n",
    "        M_rand_self_n_stars_rewards = run_n_games(max_games_number=500, our_player=q_learnt_player, opponent_epsilon=1, \\\n",
    "            our_player_new_game_epsilon=lambda game_number_n: 0, update_q_values=False)\n",
    "        M_rand_self_n_stars[n_star].append(np.average(M_rand_self_n_stars_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_rand_self_n_stars_df = pd.DataFrame(M_rand_self_n_stars)\n",
    "M_rand_self_n_stars_df.index.name = 'epochs by 250'\n",
    "M_rand_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_rand_self_n_stars_df)\n",
    "g.set_ylabel('M_rand_self_n_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "M_opt_self_n_stars_df = pd.DataFrame(M_opt_self_n_stars)\n",
    "M_opt_self_n_stars_df.index.name = 'epochs by 250'\n",
    "M_opt_self_n_stars_df.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=M_opt_self_n_stars_df)\n",
    "g.set_ylabel('M_opt_self_n_stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 (answer)\n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^âˆ—$ â€“ instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the â€˜testâ€™ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^âˆ—$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^âˆ—$ (caption\n",
    "length < 100 words).\n",
    "\n",
    "(jl, April 22 8:00 pm) **Just like in Q7, I think this is buggy** because very little learning seems to be going on. I would expect the agent here to learn at least as well as when playing a random oponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "*Question 9. What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20â€™000 games?*\n",
    "\n",
    "**TODO: Answer once sure the above code is not buggy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n",
    "*Question 10. For three board arrangements (i.e. states s), visualize Q-values of available actions (e.g.\n",
    "using heat maps). Does the result make sense? Did the agent learn the game well?\n",
    "Expected answer: A figure with 3 subplots of 3 different states with Q-values shown at available actions\n",
    "(caption length < 200 words).*\n",
    "\n",
    "**TODO: Answer once sure the above code is not buggy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep Q-Learning\n",
    "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch\n",
    "again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of\n",
    "Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is\n",
    "to approximate Q-values by a neural network instead of a look-up table as in Tabular Q-learning. For\n",
    "implementation, you can use ideas from the DQN tutorials of Keras and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Learning from experts\n",
    "Implement the DQN algorithm. To check the algorithm, run a DQN agent with a fixed and arbitrary\n",
    "$\\epsilon \\in [0,1)$ against Opt(0.5) for 20â€™000 games â€“ switch the 1st player after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "*Plot average reward and average training loss for every 250 games during training. Does\n",
    "the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure with two subplots (caption length $<$ 50 words). Specify your choice of $\\epsilon$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f371659e15671c4bf19c781ff73734bc385c323133a3685f5b4f679a2745d0"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
