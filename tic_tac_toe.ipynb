{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt_1.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against\n",
    "Opt(eps_opt) for some eps_opt âˆˆ [0, 1]. To do so, implement the Q-learning algorithm. To check the algorithm,\n",
    "run a Q-learning agent, with a fixed and arbitrary eps âˆˆ [0, 1), against Opt(0.5) for 20â€™000 games â€“ switch\n",
    "the 1st player after every game.\n",
    "Question 1. Plot average reward for every 250 games during training â€“ i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearntPlayer:\n",
    "    Q_values = {Turns[0]: {}, Turns[1]: {}}\n",
    "    prev_move = None\n",
    "    prev_grid = None\n",
    "    player = None # 'X' or 'O'\n",
    "    player_Q_values = None\n",
    "    game_env: TictactoeEnv\n",
    "\n",
    "    def __init__(self, game_env: TictactoeEnv, epsilon: float, discount_rate_gamma = 0.99, learning_rate_alpha = 0.5):\n",
    "        self.game_env = game_env\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_rate_gamma = discount_rate_gamma\n",
    "        self.learning_rate_alpha = learning_rate_alpha\n",
    "\n",
    "    def get_empty_positions(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "\n",
    "    def hash_grid(self, grid: np.ndarray):\n",
    "        return grid.tobytes()\n",
    "\n",
    "    def prepare_new_game_(self, player):\n",
    "        self.prev_move = None\n",
    "        self.prev_grid = None\n",
    "        self.curr_grid = None\n",
    "        self.curr_move = None\n",
    "        assert player == 'X' or player == 'O'\n",
    "        self.player = player\n",
    "        self.player_Q_values = self.Q_values[player]\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, possible_moves, grid_hash):\n",
    "        if len(possible_moves) == 0:\n",
    "            q_val = self.player_Q_values[grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        return max(possible_moves, key=self.player_Q_values[grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, grid_hash, possible_moves):\n",
    "        if grid_hash not in self.player_Q_values:\n",
    "            self.player_Q_values[grid_hash] = {} if len(possible_moves) > 0 else {'': 0}\n",
    "        for mv in possible_moves:\n",
    "            if mv not in self.player_Q_values[grid_hash]: self.player_Q_values[grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, grid):\n",
    "        grid_hash = self.hash_grid(grid)\n",
    "        # Get moves\n",
    "        possible_moves = player_opt_1.empty(grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        if random.random() >= self.epsilon:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self.curr_grid = grid\n",
    "        self.curr_move = chosen_move\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_grid, prev_move = self.prev_grid, self.prev_move\n",
    "        self.prev_grid = self.curr_grid\n",
    "        self.prev_move = self.curr_move\n",
    "        self.curr_grid, self.curr_move = None, None\n",
    "        if prev_grid is not None and prev_move is not None:\n",
    "            new_grid_hash = self.hash_grid(new_grid)\n",
    "            prev_grid_hash = self.hash_grid(prev_grid)\n",
    "            reward = self.game_env.reward(self.player)\n",
    "            # Get max_a (Q(S', a))\n",
    "            possible_moves_s_dash = player_opt_1.empty(new_grid)\n",
    "            self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "            max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "            max_q_value = self.player_Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "            # Update according to Q-learning formula\n",
    "            prev_q_val = self.player_Q_values[prev_grid_hash][prev_move]\n",
    "            self.player_Q_values[prev_grid_hash][prev_move] += self.learning_rate_alpha*(reward + self.discount_rate_gamma*max_q_value - prev_q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "rewards = [None for x in range(max_games)]\n",
    "q_learnt_player = QLearntPlayer(env, epsilon=0.2)\n",
    "\n",
    "for game in range(max_games):\n",
    "    if game % 1000 == 0:\n",
    "        print('Game ', game, ' begins.')\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    opponent =  OptimalPlayer(epsilon=0.5, player=Turns[0])\n",
    "    our_player = q_learnt_player.prepare_new_game_(Turns[1])\n",
    "\n",
    "    for turn in range(9):\n",
    "        opponent_turn = env.current_player == opponent.player\n",
    "        if opponent_turn:\n",
    "            chosen_move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            chosen_move = our_player.choose_move_(grid)\n",
    "\n",
    "        grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "        if opponent_turn:\n",
    "            q_learnt_player.update_q_values_(grid)\n",
    "        if end:\n",
    "            q_learnt_player.update_q_values_(grid)\n",
    "            rewards[game] = env.reward(our_player.player)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avgs = []\n",
    "total_wins = sum(1 if rew ==1 else 0 for rew in rewards)\n",
    "print('Our agent won {} times'.format(total_wins))\n",
    "for x in range(0,len(rewards), 250):\n",
    "    lower_index = x\n",
    "    upper_index = min(x+250, len(rewards)-1)\n",
    "    slice = rewards[lower_index:upper_index]\n",
    "    avgs.append(sum(slice)/len(slice))\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(avgs)\n",
    "plt.xticks(ticks=range(len(avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Plot average reward for every 250 games during training. Does decreasing epsilon help training\n",
    "compared to having a fixed epsilon? What is the effect of nâˆ—?\n",
    "Expected answer: A figure showing average reward over time for different values of nâˆ— (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_games = 20000\n",
    "rewards = {}\n",
    "\n",
    "min_epsilon = 0.1\n",
    "max_epsilon = 0.8\n",
    "calc_epsilon = lambda game_number_n, n_star: max(min_epsilon, max_epsilon*(1-(game_number_n/n_star)))\n",
    "\n",
    "q_learnt_player = QLearntPlayer(env, epsilon=max_epsilon)\n",
    "\n",
    "n_stars =  np.linspace(1, 40000, num=4) # Includes 1 and 40000\n",
    "\n",
    "for n_star in n_stars:\n",
    "    print('Current n_star = {}'.format(n_star))\n",
    "    rewards[n_star] = [None for x in range(max_games)]\n",
    "\n",
    "    for game in range(max_games):\n",
    "        if game % 5000 == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        Turns = Turns[np.random.permutation(2)]\n",
    "        opponent =  OptimalPlayer(epsilon=0.5, player=Turns[0])\n",
    "        our_player = q_learnt_player.prepare_new_game_(Turns[1])\n",
    "        our_player.epsilon = calc_epsilon(game_number_n=game, n_star=n_star)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = player_opt_1.act(grid)\n",
    "            else:\n",
    "                chosen_move = our_player.choose_move_(grid)\n",
    "\n",
    "            grid, end, winner = env.step(chosen_move, print_grid=False)\n",
    "\n",
    "            if opponent_turn:\n",
    "                q_learnt_player.update_q_values_(grid)\n",
    "            if end:\n",
    "                q_learnt_player.update_q_values_(grid)\n",
    "                rewards[n_star][game] = env.reward(our_player.player)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "avgs = {n_star: [] for n_star in n_stars}\n",
    "for x in range(0,max_games, 250):\n",
    "    lower_index = x\n",
    "    upper_index = min(x+250, max_games-1)\n",
    "    for n_star in n_stars:\n",
    "        slice = rewards[n_star][lower_index:upper_index]\n",
    "        avgs[n_star].append(sum(slice)/len(slice))\n",
    "\n",
    "q2_data = pd.DataFrame(avgs)\n",
    "q2_data.index.name = 'epochs by 250'\n",
    "q2_data.transpose().index.name = 'n_star'\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average rewards per 250 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f371659e15671c4bf19c781ff73734bc385c323133a3685f5b4f679a2745d0"
  },
  "kernelspec": {
   "display_name": "ANN_env_2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
