{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Artificial Neural Networks Notebook} $\n",
    "### $\\text{Mini-project 1: Tic Tac Toe}$\n",
    "#### $Ji\\v{r}\\'{i}$ $Lhotka$ $and$ $Alexandre$ $Variengien$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and global variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42) #set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "DEBUG = False\n",
    "MAX_GAMES_DEFAULT = 20_000\n",
    "\n",
    "GAMES_FOR_EVAL = 500 # It should be 500 for the final version\n",
    "\n",
    "env = TictactoeEnv()\n",
    "Turns = np.array(['X','O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_id(): \n",
    "    \"\"\"Generate a random string of numbers. This function is \n",
    "    used to generate the names of the plots saved to svg to avoid overwriting\n",
    "    past files.\"\"\"\n",
    "    return \"_\"+str(random.random())[3:] \n",
    "\n",
    "def grid_to_string(bts, is_buffer = True):\n",
    "    _grid = np.reshape(np.frombuffer(bts),(3,3)) if is_buffer else bts\n",
    "    str_rep = ''\n",
    "    value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "    for i in range(3):\n",
    "        str_rep +='|'\n",
    "        for j in range(3):\n",
    "            str_rep += value2player[int(_grid[i,j])] + (' ' if j<2 else '')\n",
    "        str_rep+='|\\n'\n",
    "    str_rep+='\\n'\n",
    "    return str_rep\n",
    "\n",
    "def print_Q_val_with_moves(q_vals, descending=False, only_vals = None):\n",
    "    qv_temp = {grid_to_string(k): q_vals[k] for k in q_vals if any(map(lambda a: q_vals[k][a] != 0, q_vals[k]))}.items()\n",
    "    count_free_squares = lambda grid: sum([x=='-' for x in grid])\n",
    "    if only_vals is not None:\n",
    "        qv_temp = list(filter(lambda k: count_free_squares(k[0]) == only_vals,qv_temp))\n",
    "    for (i,j) in sorted(qv_temp, key=lambda k: count_free_squares(k[0]), reverse=descending):\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition for the normal Q-Learning RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_empty_positions_from_grid(_grid):\n",
    "    '''return all empty positions'''\n",
    "    avail = []\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if _grid[pos] == 0:\n",
    "            avail.append(pos)\n",
    "    return avail\n",
    "\n",
    "class BasePlayer:\n",
    "    def get_empty_positions(self, _grid):\n",
    "        return get_empty_positions_from_grid(_grid)\n",
    "\n",
    "    def hash_grid(self, _grid: np.ndarray):\n",
    "        return _grid.tobytes()\n",
    "\n",
    "class QLearntPlayer(BasePlayer):\n",
    "    def __init__(self, _epsilon: float, _discount_rate_gamma = 0.99, _learning_rate_alpha = 0.05):\n",
    "        super()\n",
    "        self._epsilon = _epsilon\n",
    "        self._discount_rate_gamma = _discount_rate_gamma\n",
    "        self._learning_rate_alpha = _learning_rate_alpha\n",
    "        self._Q_values = {}\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        self._player = None # 'X' or 'O'\n",
    "\n",
    "    def prepare_new_game_(self, _player):\n",
    "        self._curr_grid = None\n",
    "        self._curr_move = None\n",
    "        assert _player == 'X' or _player == 'O'\n",
    "        self._player = _player\n",
    "        return self\n",
    "\n",
    "    def get_max_val_action(self, _possible_moves, _grid_hash):\n",
    "        if len(_possible_moves) == 0:\n",
    "            q_val = self._Q_values[_grid_hash]['']\n",
    "            assert type(q_val) is int\n",
    "            return ''\n",
    "        # Shuffle moves to prevent bias towards choosing the first thing in the list\n",
    "        # This is important especially at the beginning when all the q-vals are 0\n",
    "        # And we are therefore biased towards choosing the first avaliable move, e.g. (0,0) in the\n",
    "        # starting position\n",
    "        random.shuffle(_possible_moves)\n",
    "        return max(_possible_moves, key=self._Q_values[_grid_hash].get)\n",
    "\n",
    "    def init_q_values_(self, _grid_hash, _possible_moves):\n",
    "        if _grid_hash not in self._Q_values:\n",
    "            self._Q_values[_grid_hash] = {} if len(_possible_moves) > 0 else {'': 0}\n",
    "        for mv in _possible_moves:\n",
    "            if mv not in self._Q_values[_grid_hash]: self._Q_values[_grid_hash][mv] = 0\n",
    "\n",
    "    def choose_move_(self, _grid):\n",
    "        grid_hash = self.hash_grid(_grid)\n",
    "        # Get moves\n",
    "        possible_moves = self.get_empty_positions(_grid)\n",
    "        assert len(possible_moves) > 0\n",
    "        # Init Q_values\n",
    "        self.init_q_values_(grid_hash, possible_moves)\n",
    "        # Choose move (eps.greedy)\n",
    "        random_sample = random.random()\n",
    "        play_best_move = random_sample >= self._epsilon\n",
    "\n",
    "\n",
    "        if play_best_move:\n",
    "            chosen_move = self.get_max_val_action(possible_moves, grid_hash)\n",
    "        else:\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "        self._curr_grid = _grid\n",
    "        self._curr_move = chosen_move\n",
    "        if DEBUG:\n",
    "            print('-----------------------------------')\n",
    "            print('Current position: ', '\\n' + grid_to_string(_grid, False))\n",
    "            print('Current Q-vals', self._Q_values[grid_hash])\n",
    "            print('Random sample ', random_sample, ' _epsilon ', self._epsilon, ' hence I chose ', \\\n",
    "                '*best*' if play_best_move else '*random*', ' move: ', chosen_move )\n",
    "            print('-----------------------------------')\n",
    "\n",
    "        return chosen_move\n",
    "\n",
    "    def update_q_values_(self, new_grid, game_over, _reward):\n",
    "        \"\"\"\n",
    "        update Q values by Q-learning formula.\n",
    "\n",
    "        new_grid ~ S' in the formula\n",
    "        \"\"\"\n",
    "        prev_move, prev_grid = self._curr_move, self._curr_grid\n",
    "        self._curr_grid, self._curr_move = None, ''\n",
    "        new_grid_hash = self.hash_grid(new_grid)\n",
    "        prev_grid_hash = self.hash_grid(prev_grid)\n",
    "        # Get max_a (Q(S', a))\n",
    "        possible_moves_s_dash = [] if game_over else self.get_empty_positions(new_grid)\n",
    "        self.init_q_values_(new_grid_hash, possible_moves_s_dash)\n",
    "        max_val_action = self.get_max_val_action(possible_moves_s_dash, new_grid_hash)\n",
    "        max_q_value = self._Q_values[new_grid_hash][max_val_action]\n",
    "\n",
    "        if DEBUG:\n",
    "            print('*** UPDATING Q VALS ****')\n",
    "            game_over and print('*Game is over*')\n",
    "            print('Prev_grid: ', '\\n' + grid_to_string(prev_grid, False))\n",
    "            print('Prev_move: ', prev_move)\n",
    "            print('new_grid: ', '\\n' + grid_to_string(new_grid, False))\n",
    "            print('max_val_action: ', max_val_action)\n",
    "            print('Q-vals before: ', self._Q_values[prev_grid_hash])\n",
    "\n",
    "        # Update according to Q-learning formula\n",
    "        prev_q_val = self._Q_values[prev_grid_hash][prev_move]\n",
    "        self._Q_values[prev_grid_hash][prev_move] += self._learning_rate_alpha*(_reward + self._discount_rate_gamma*max_q_value - prev_q_val)\n",
    "        if DEBUG:\n",
    "            print('_reward: ', _reward, 'max-q-val', max_q_value, 'discount', self._discount_rate_gamma,\\\n",
    "                'self._learning_rate_alpha',self._learning_rate_alpha)\n",
    "            print('Q-vals after: ', self._Q_values[prev_grid_hash])\n",
    "            print('*****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition of the DQN agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining state handling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SYMBOL_TO_INT = {\"X\": 1, \"O\": -1}\n",
    "\n",
    "def state_to_torch(game_state: np.ndarray, player_symbol):\n",
    "    uniformed_grid = SYMBOL_TO_INT[player_symbol]*game_state # grid where all 1 are the player and -1 the oponent, no matter the turn\n",
    "    torch_no_batch =  torch.cat((torch.from_numpy((uniformed_grid == 1).astype(np.float32)).view(1,3,3,1), torch.from_numpy((uniformed_grid == -1).astype(np.float32)).view(1,3,3,1)), dim=3).to(device)\n",
    "    return torch_no_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell's code is largely taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html?highlight=huber\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # \" Once a bounded length deque is full, when new items are added, a corresponding number\n",
    "        # of items are discarded from the opposite end. \" (https://docs.python.org/3/library/collections.html#collections.deque)\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, _new_memory: Transition):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(_new_memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net =  nn.Sequential(\n",
    "            nn.Linear(18, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 9))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        batch_size = inp.size(0)\n",
    "        input_in_1D = inp.view(batch_size,-1)\n",
    "        output = self.net(input_in_1D)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Player(BasePlayer):\n",
    "    INDEX_TO_POSITIONS = BasePlayer().get_empty_positions(TictactoeEnv().grid)\n",
    "    POSITIONS_TO_INDEX = {position: index for index, position in enumerate(INDEX_TO_POSITIONS)}\n",
    "    LEARNING_RATE = 5e-4 # Given in the instructions PDF\n",
    "    DISCOUNT_RATE_GAMMA = 0.99\n",
    "    DEFAULT_BATCH_SIZE = 64\n",
    "    DEFAULT_BUFFER_SIZE = 10_000\n",
    "    TARGET_NEW_UPDATE_FREQUENCY = 500\n",
    "\n",
    "    def __init__(self, _epsilon: float, _batch_size = DEFAULT_BATCH_SIZE, _replay_buffer_size = DEFAULT_BUFFER_SIZE):\n",
    "        self._policy_net = DQNet()\n",
    "        self._target_net = DQNet()\n",
    "        self._target_net.load_state_dict(self._policy_net.state_dict())\n",
    "        self._replay_buffer = ReplayMemory(_replay_buffer_size)\n",
    "        self._optimizer = torch.optim.Adam(self._policy_net.parameters(), lr=DQN_Player.LEARNING_RATE)\n",
    "        self._criterion = nn.SmoothL1Loss() # \"When delta is set to 1, this loss is equivalent to SmoothL1Loss.\" (PyTorch HuberLoss documentation)\n",
    "        self._epsilon = _epsilon\n",
    "        self._latest_memory = {'state': None, 'action': None, 'next_state': None, 'reward': None}\n",
    "        self._player = None # 'X' or 'O'\n",
    "        self._game_count = 0\n",
    "        self._batch_size = _batch_size\n",
    "        self._replay_buffer_size = _replay_buffer_size\n",
    "        self.losses = []\n",
    "\n",
    "    def get_loss_and_reset_log(self, average=True ):\n",
    "        if average: \n",
    "            avg = np.average(self.losses)\n",
    "            self.losses = []\n",
    "            return avg\n",
    "        else:\n",
    "            last_losses = self.losses.copy()\n",
    "            self.losses = []\n",
    "            return last_losses\n",
    "    def prepare_new_game_(self, _player):\n",
    "        assert _player == 'X' or _player == 'O'\n",
    "        self._player = _player\n",
    "        self._game_count += 1\n",
    "        return self\n",
    "\n",
    "    def get_q_values_from_network(self, _torch_grid):\n",
    "        with torch.no_grad():\n",
    "            net_q_vals = self._policy_net(_torch_grid)\n",
    "            # This is slow and it would be easier to just take the max but I implement it like this for easier debugging\n",
    "            # Also this is consistent with previous Q-value representation, which will allow us to reuse debugging functions\n",
    "            numpy_q_vals = net_q_vals.numpy().reshape(9)\n",
    "            dict_q_vals = {move: numpy_q_vals[move_index] for (move_index, move) in enumerate(DQN_Player.INDEX_TO_POSITIONS)} \n",
    "            return dict_q_vals\n",
    "\n",
    "    def get_max_val_action(self, _q_values):\n",
    "        return max(_q_values, key=_q_values.get)\n",
    "\n",
    "    def choose_move_(self, _grid):\n",
    "        torch_grid = state_to_torch(_grid, self._player)\n",
    "\n",
    "        random_sample = random.random()\n",
    "        play_best_move = random_sample >= self._epsilon\n",
    "        if play_best_move:\n",
    "            q_values = self.get_q_values_from_network(_torch_grid=torch_grid)\n",
    "            chosen_move = self.get_max_val_action(_q_values=q_values)\n",
    "        else:\n",
    "            possible_moves = self.get_empty_positions(_grid)\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "\n",
    "        self._latest_memory['state'] = torch_grid\n",
    "        self._latest_memory['action'] = torch.tensor([DQN_Player.POSITIONS_TO_INDEX[chosen_move]])\n",
    "\n",
    "        if DEBUG:\n",
    "            if not play_best_move:\n",
    "                q_values = self.get_q_values_from_network(_torch_grid=torch_grid) # ensure q_values are defined\n",
    "            print('-----------------------------------')\n",
    "            print('Current position: ', '\\n' + grid_to_string(_grid, False))\n",
    "            print('Current Q-vals', q_values)\n",
    "            print('Random sample ', random_sample, ' _epsilon ', self._epsilon, ' hence I chose ', \\\n",
    "                '*best*' if play_best_move else '*random*', ' move: ', chosen_move )\n",
    "            print('-----------------------------------')\n",
    "\n",
    "        return chosen_move\n",
    "\n",
    "    def reset_latest_memory_(self):\n",
    "        self._latest_memory['state'] = None\n",
    "        self._latest_memory['action'] = None\n",
    "        self._latest_memory['next_state'] = None\n",
    "        self._latest_memory['reward'] = None\n",
    "\n",
    "    def create_memory_(self, _next_state, _reward):\n",
    "        self._latest_memory['next_state'] = _next_state\n",
    "        self._latest_memory['reward'] = torch.tensor([_reward])\n",
    "        new_memory = Transition(**self._latest_memory)\n",
    "        self._replay_buffer.push(new_memory)\n",
    "        self.reset_latest_memory_()\n",
    "\n",
    "    def learn_from_memories_(self):\n",
    "        if len(self._replay_buffer) < self._batch_size:\n",
    "            return\n",
    "        replay_memories = self._replay_buffer.sample(self._batch_size)\n",
    "\n",
    "        # next few lines inspired by https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training-loop\n",
    "        batch = Transition(*zip(*replay_memories))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s != '', \\\n",
    "            batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = state_batch[non_final_mask]\n",
    "\n",
    "        _temp = self._policy_net(state_batch)\n",
    "\n",
    "        pre_move_q_vals = _temp.gather(1, action_batch.view(action_batch.size(0),1))\n",
    "\n",
    "        post_move_q_vals = torch.zeros(self._batch_size)\n",
    "\n",
    "        # Don't call net if there are no non_final states in the batch\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            post_move_q_vals[non_final_mask] = self._target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_post_move_q_vals = (post_move_q_vals * self.DISCOUNT_RATE_GAMMA) + reward_batch\n",
    "\n",
    "\n",
    "        loss = self._criterion(pre_move_q_vals, expected_post_move_q_vals.unsqueeze(1))\n",
    "\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        \n",
    "        self.losses.append(float(loss.detach().numpy()))\n",
    "\n",
    "        if self._game_count % self.TARGET_NEW_UPDATE_FREQUENCY == 0:\n",
    "            self._target_net.load_state_dict(self._policy_net.state_dict())\n",
    "\n",
    "    def update_q_values_(self, new_grid, game_over, _reward):\n",
    "        \"\"\"\n",
    "        same signature and name as in QLearntPlayer in order to keep the same interface,\n",
    "        even though another name would perhaps be more appropriate here\n",
    "        \"\"\"\n",
    "        # If the next state is a final state, save '' instead: we need to signify the state is final\n",
    "        # as the target network cannot predict the q-value of best action there since in a final state,\n",
    "        # no action can be taken (we just give it Q-val of 0 instead)\n",
    "        next_state = state_to_torch(new_grid, self._player) if not game_over else ''\n",
    "        self.create_memory_(_next_state=next_state, _reward=_reward)\n",
    "        self.learn_from_memories_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiment function\n",
    "Those functions runs experiments for both the normal Q-learning agent and the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run games against $\\texttt{Opt(Ɛ)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_opt(_max_games_number, _our_player, _opponent_epsilon, \n",
    "                            _our_player_new_game_epsilon, _update_q_values, \n",
    "                            _progress_print=None, _throw_error_on_illegal_moves = True):\n",
    "                            \n",
    "    _rewards = [None for _ in range(_max_games_number)]\n",
    "    _turns = ['X','O']\n",
    "    opponent =  OptimalPlayer(epsilon=_opponent_epsilon, player=_turns[0])\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        env.reset()\n",
    "        _current_grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent.player = _turns[0]\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent.player != _our_player._player\n",
    "        _our_player._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        if (_progress_print and game % _progress_print == 0) or DEBUG:\n",
    "            print('Game ', game, ' begins.')\n",
    "            if DEBUG:\n",
    "                print('We play: ', _our_player._player)\n",
    "                input('awaiting input: ')\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent.player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.act(_current_grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(_current_grid)\n",
    "\n",
    "            # This if branch was added to make this work for DQN player.\n",
    "            if (not _throw_error_on_illegal_moves) and (chosen_move not in get_empty_positions_from_grid(_current_grid)):\n",
    "                _current_grid = None\n",
    "                _end = True\n",
    "                _reward = -1 # You lose if you play an illegal move\n",
    "\n",
    "            else:\n",
    "                _current_grid, _end, winner = env.step(chosen_move, print_grid=False)\n",
    "                _reward = env.reward(_our_player._player)\n",
    "\n",
    "            if _update_q_values and (opponent_turn and turn > 0 or _end):\n",
    "                _our_player.update_q_values_(_current_grid, game_over=_end, _reward=_reward)\n",
    "            if _end:\n",
    "                _rewards[game] = _reward\n",
    "                break\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run games by self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_games_against_self(_max_games_number, _our_player, _our_player_new_game_epsilon, _update_q_values, _progress_print=None, _throw_error_on_illegal_moves = True):\n",
    "    _rewards = {'our_player': [None for _ in range(_max_games_number)],'opponent': [None for _ in range(_max_games_number)]}\n",
    "    _turns = ['X','O']\n",
    "\n",
    "    \n",
    "    if isinstance(_our_player,QLearntPlayer):\n",
    "\n",
    "        opponent = QLearntPlayer(_our_player._epsilon,_our_player._discount_rate_gamma, _our_player._learning_rate_alpha)\n",
    "\n",
    "        # Ensure they share Q_values by sharing the memory pointers\n",
    "        opponent._Q_values = _our_player._Q_values\n",
    "\n",
    "    if isinstance(_our_player, DQN_Player):\n",
    "\n",
    "        opponent = DQN_Player(_epsilon=_our_player._epsilon,\n",
    "                               _batch_size= _our_player._batch_size,\n",
    "                               _replay_buffer_size= _our_player._replay_buffer_size)\n",
    "\n",
    "        opponent._policy_net = _our_player._policy_net\n",
    "        opponent._target_net = _our_player._target_net\n",
    "        opponent._replay_buffer = _our_player._replay_buffer\n",
    "        opponent._optimizer = _our_player._optimizer\n",
    "\n",
    "        #we use memory links to share common data structures \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    for game in range(_max_games_number):\n",
    "        if _progress_print and game % _progress_print == 0:\n",
    "            print('Game ', game, ' begins.')\n",
    "        env.reset()\n",
    "        _current_grid, _, __ = env.observe()\n",
    "        _turns = _turns[::-1] # Reverse after every game to ensure both sides played equally\n",
    "        opponent = opponent.prepare_new_game_(_turns[0])\n",
    "        _our_player = _our_player.prepare_new_game_(_turns[1])\n",
    "        assert opponent._player != _our_player._player\n",
    "        _our_player._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "        opponent._epsilon = _our_player_new_game_epsilon(game_number_n=game)\n",
    "\n",
    "        for turn in range(9):\n",
    "            opponent_turn = env.current_player == opponent._player\n",
    "            if opponent_turn:\n",
    "                chosen_move = opponent.choose_move_(_current_grid)\n",
    "            else:\n",
    "                chosen_move = _our_player.choose_move_(_current_grid)\n",
    "\n",
    "            if (not _throw_error_on_illegal_moves) and (chosen_move not in get_empty_positions_from_grid(_current_grid)):\n",
    "                # This branch is used for DQN \n",
    "                _current_grid = None\n",
    "                _end = True\n",
    "                # Whoever makes an illegal move loses and gets -1 reward, the other player gets a reward\n",
    "                # of 0 not to incentivize moves that make the other misplay\n",
    "                _our_player_reward = 0 if opponent_turn else -1\n",
    "                _opponent_reward = -1 if opponent_turn else 0\n",
    "\n",
    "            else:\n",
    "                _current_grid, _end, winner = env.step(chosen_move, print_grid=False)\n",
    "                _our_player_reward = env.reward(_our_player._player)\n",
    "                _opponent_reward = env.reward(opponent._player)\n",
    "\n",
    "            if _end:\n",
    "                _update_q_values and _our_player.update_q_values_(_current_grid, game_over = _end, _reward=_our_player_reward)\n",
    "                _update_q_values and opponent.update_q_values_(_current_grid, game_over = _end, _reward=_opponent_reward)\n",
    "                _rewards['our_player'][game] = _our_player_reward\n",
    "                _rewards['opponent'][game] = _opponent_reward\n",
    "                break\n",
    "            elif turn !=0 and _update_q_values:\n",
    "                if opponent_turn:\n",
    "                    _our_player.update_q_values_(_current_grid, game_over = _end, _reward=_our_player_reward)\n",
    "                else:\n",
    "                    opponent.update_q_values_(_current_grid, game_over = _end, _reward=_opponent_reward)\n",
    "    return _rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "def visualize_qvalues(agent, grids):\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    fig, axs = plt.subplots( nrows=1,ncols = len(grids), figsize =(30,10))\n",
    "\n",
    "    for i,grid in enumerate(grids):\n",
    "        if isinstance(agent, QLearntPlayer):\n",
    "            qvals = agent._Q_values[agent.hash_grid(grid)]\n",
    "\n",
    "        elif isinstance(agent, DQN_Player):\n",
    "            if np.sum(grid) == 0:\n",
    "                turn = 'X'\n",
    "            elif np.sum(grid) == 1:\n",
    "                turn = 'O'\n",
    "            else:\n",
    "                raise ValueError(\"Grid impossible to exist\")\n",
    "\n",
    "            torch_grid = state_to_torch(grid, turn)            \n",
    "            qvals = agent.get_q_values_from_network(_torch_grid=torch_grid)\n",
    "        else:\n",
    "            raise ValueError(\"Bad agent type!\")\n",
    "        val_on_grid = np.full((3, 3), np.inf)\n",
    "        for (x,y) in qvals.keys():\n",
    "            val_on_grid[x][y] = qvals[(x,y)]\n",
    "\n",
    "        im = axs[i].matshow(val_on_grid, cmap=\"PiYG\", vmax=0.5, vmin=-0.5)\n",
    "        ofst = 0.15\n",
    "        grid = np.transpose(grid)\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                if grid[x][y] == 1:\n",
    "                    axs[i].text(x-ofst, y+ofst,\"X\", fontsize=30)\n",
    "                elif grid[x][y] == -1:\n",
    "                    axs[i].text(x-ofst, y+ofst,\"O\", fontsize=30)\n",
    "        plt.colorbar(im, ax=axs[i], label=\"Q-values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q1_epsilon = 0.1 # Chosen because they use this in Q2 so this will allow us to nicely compare\n",
    "\n",
    "q1_max_games = MAX_GAMES_DEFAULT\n",
    "q1_q_learnt_player = QLearntPlayer(_epsilon=q1_epsilon)\n",
    "print_Q_val_with_moves(q1_q_learnt_player._Q_values)\n",
    "q1_avgs = []\n",
    "q1_rewards = []\n",
    "q1_total_wins = 0\n",
    "\n",
    "for game_epoch in range(q1_max_games//250):\n",
    "    if game_epoch % 10 == 0:\n",
    "        print('Game ', game_epoch*250, ' begins.')\n",
    "    run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q1_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q1_epsilon, _update_q_values=True)\n",
    "    q1_rewards+=run_rewards\n",
    "    q1_avgs.append(np.average(run_rewards))\n",
    "    q1_total_wins += sum(1 if rew ==1 else 0 for rew in run_rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.plot(q1_avgs)\n",
    "plt.xticks(ticks=range(len(q1_avgs)), labels=[str(x*250 // 1000) if x*250 % 1000 == 0 else '' for x in range(len(q1_avgs))])\n",
    "plt.ylabel('Average reward per 250 episodes')\n",
    "plt.xlabel('Episode (thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. \n",
    "Plot average reward for every 250 games during training. Does decreasing epsilon help training\n",
    "compared to having a fixed epsilon? What is the effect of n∗?\n",
    "Expected answer: A figure showing average reward over time for different values of n∗ (caption length < 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_max_games = MAX_GAMES_DEFAULT\n",
    "q2_n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "q2_rewards = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_avgs = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_M_opt = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_M_rand = {n_star_q2: [] for n_star_q2 in q2_n_stars}\n",
    "q2_total_wins = {n_star_q2: 0 for n_star_q2 in q2_n_stars}\n",
    "q2_epoch_size = 250\n",
    "q2_players = {}\n",
    "\n",
    "q2_min_epsilon = 0.1\n",
    "q2_max_epsilon = 0.8\n",
    "def q2_calc_epsilon_factory(n_star_q2, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*q2_epoch_size + game_number_n\n",
    "            return max(q2_min_epsilon, q2_max_epsilon*(1-(real_game_number/n_star_q2)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q2 in q2_n_stars:\n",
    "    q2_starting_epsilon  = q2_calc_epsilon_factory(n_star_q2, q2_epoch_size, 0)(0)\n",
    "    q2_q_learnt_player = QLearntPlayer(_epsilon=q2_starting_epsilon)\n",
    "    q2_players[n_star_q2] = q2_q_learnt_player\n",
    "    print('Current n_star_q2 = {}'.format(n_star_q2))\n",
    "\n",
    "    for game_epoch in range(q2_max_games//q2_epoch_size):\n",
    "        q2_calc_epsilon = q2_calc_epsilon_factory(n_star_q2=n_star_q2, epoch_size=q2_epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q2_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        q2_run_rewards = run_n_games_against_opt(_max_games_number=q2_epoch_size, _our_player=q2_q_learnt_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=q2_calc_epsilon, _update_q_values=True)\n",
    "        q2_avgs[n_star_q2].append(np.average(q2_run_rewards))\n",
    "        q2_rewards[n_star_q2] += q2_run_rewards\n",
    "        q2_total_wins[n_star_q2] += sum(1 if rew ==1 else 0 for rew in q2_run_rewards)\n",
    "\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q2_M_opt_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q2_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q2_M_opt[n_star_q2].append(np.average(q2_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q2_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q2_M_rand[n_star_q2].append(np.average(M_rand_rewards))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_data = pd.DataFrame(q2_avgs, index =[250*i for i in range(20_000//250)] )\n",
    "\n",
    "q2_data.transpose().index.name = 'n star'\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(24,12)})\n",
    "g = sns.lineplot(data=q2_data)\n",
    "g.set_ylabel('Average reward per game during training over 250 games')\n",
    "g.set_xlabel('Training game')\n",
    "plt.savefig(\"Q2\"+get_random_id()+\".svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "episodes = [250*i for i in range(MAX_GAMES_DEFAULT//250)]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, M_Rand_list) in enumerate(q2_M_rand.items()):\n",
    "    plt.plot(episodes,M_Rand_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, M_Opt_list) in enumerate(q2_M_opt.items()):\n",
    "    plt.plot(episodes, M_Opt_list,linestyle=\"-.\",color=colors[i], label = f\"{n_star}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q3\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Choose the best value of $n^∗$ that you found in the previous section. Run Q-learning against Opt($\\epsilon_{opt}$) for\n",
    "different values of $\\epsilon_{opt}$ for 20’000 games – switch the 1st player after every game. Choose several values\n",
    "of $\\epsilon_{opt}$ from a reasonably wide interval between 0 to 1 – particularly, include $\\epsilon_{opt}$ = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_star = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_max_games = MAX_GAMES_DEFAULT\n",
    "q4_epoch_size = 250\n",
    "\n",
    "q4_eps_opts = np.linspace(0,1,num=5)\n",
    "\n",
    "q4_rewards_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "q4_M_opt_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "q4_M_rand_eps = {eps_opt: [] for eps_opt in q4_eps_opts}\n",
    "\n",
    "q4_min_epsilon = 0.1\n",
    "q4_max_epsilon = 0.8\n",
    "\n",
    "\n",
    "def q4_calc_epsilon_factory_eps(epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q4_min_epsilon, q4_max_epsilon*(1-(real_game_number/best_n_star)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for eps_opt in q4_eps_opts:\n",
    "    q4_starting_epsilon  = q4_calc_epsilon_factory_eps(q4_epoch_size, 0)(0)\n",
    "    q4_q_learnt_player = QLearntPlayer(_epsilon=q4_starting_epsilon)\n",
    "    print('Current eps_opt = {}'.format(eps_opt))\n",
    "\n",
    "    for game_epoch in range(q4_max_games//q4_epoch_size):\n",
    "        q4_calc_epsilon = q4_calc_epsilon_factory_eps(epoch_size=q4_epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q4_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 4)\n",
    "        q4_run_rewards = run_n_games_against_opt(_max_games_number=q4_epoch_size, _our_player=q4_q_learnt_player, _opponent_epsilon=eps_opt, \\\n",
    "            _our_player_new_game_epsilon=q4_calc_epsilon, _update_q_values=True)\n",
    "        q4_rewards_eps[eps_opt] += q4_run_rewards\n",
    "\n",
    "        # Run 500 games for q4_M_opt_eps calculation\n",
    "        q4_M_opt_eps_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q4_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q4_M_opt_eps[eps_opt].append(np.average(q4_M_opt_eps_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q4_M_rand_eps calculation\n",
    "        q4_M_rand_eps_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q4_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q4_M_rand_eps[eps_opt].append(np.average(q4_M_rand_eps_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "episodes = [250*i for i in range(MAX_GAMES_DEFAULT//250)]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(eps, q4_M_rand_eps_list) in enumerate(q4_M_rand_eps.items()):\n",
    "    plt.plot(episodes,q4_M_rand_eps_list,linestyle=\"solid\", color=colors[i], label = f\"{eps}\")\n",
    "\n",
    "for i,(eps, q4_M_opt_eps_list) in enumerate(q4_M_opt_eps.items()):\n",
    "    plt.plot(episodes, q4_M_opt_eps_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q4\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 5\n",
    "What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?\n",
    "\n",
    "The highest value of $M_{opt}$ is achieved by $\\epsilon=0.0$ and $\\epsilon=0.25$ and it is the value 0 which is the best that we can hope for against  $M_{opt}$. The highest value of $M_{rand}$ is near $0.9$ achieved by  $\\epsilon=0.75$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in q4_M_rand_eps.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q4_M_rand_eps[eps])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Learning by self-practice\n",
    "In this section, your are supposed to ask whether Q-learning can learn to play Tic Tac Toe by only\n",
    "playing against itself. For different values of $\\epsilon \\in [0, 1)$, run a Q-learning agent against itself for 20’000\n",
    "games – i.e. both players use the same set of Q-values and update the same set of Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 \n",
    " After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for different\n",
    "values of $\\epsilon \\in [0, 1)$. Does the agent learn to play Tic Tac Toe? What is the effect of $\\epsilon$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of $\\epsilon \\in [0, 1)$ (caption\n",
    "length < 100 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7_max_games = MAX_GAMES_DEFAULT\n",
    "q7_epoch_size = 250\n",
    "\n",
    "q7_eps_selfs = np.linspace(0,0.99,num=5)\n",
    "\n",
    "q7_M_opt_self = {eps_opt: [] for eps_opt in q7_eps_selfs}\n",
    "q7_M_rand_self = {eps_opt: [] for eps_opt in q7_eps_selfs}\n",
    "\n",
    "q7_min_epsilon = 0.1\n",
    "q7_max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "for eps_s in q7_eps_selfs:\n",
    "    q7_q_learnt_player = QLearntPlayer(_epsilon=q7_min_epsilon)\n",
    "    print('Current eps_s = {}'.format(eps_s))\n",
    "\n",
    "    for game_epoch in range(q7_max_games//q7_epoch_size):\n",
    "        q7_calc_epsilon = lambda game_number_n: eps_s\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q7_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 7)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=q7_epoch_size, _our_player=q7_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=q7_calc_epsilon, _update_q_values=True)\n",
    "\n",
    "        # Run 500 games for q7_M_opt_self calculation\n",
    "        q7_M_opt_self_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q7_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q7_M_opt_self[eps_s].append(np.average(q7_M_opt_self_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q7_M_rand_self calculation\n",
    "        q7_M_rand_self_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q7_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q7_M_rand_self[eps_s].append(np.average(q7_M_rand_self_rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = [250*i for i in range(MAX_GAMES_DEFAULT//250)]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, q7_M_rand_eps_list) in enumerate(q7_M_rand_self.items()):\n",
    "    plt.plot(episodes,q7_M_rand_eps_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, q7_M_opt_eps_list) in enumerate(q7_M_opt_self.items()):\n",
    "    plt.plot(episodes, q7_M_opt_eps_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q7.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 \n",
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^∗$ – instead of fixing $\\epsilon$.\n",
    "Question 8. After every 250 games during training, compute the ‘test’ $M_{opt}$ and $M_{rand}$ for your agents.\n",
    "Does decreasing $\\epsilon$ help training compared to having a fixed $\\epsilon$? What is the effect of $n^∗$?\n",
    "Expected answer: A figure showing $M_{opt}$ and $M_{rand}$ over time for different values of speeds of $n^∗$ (caption\n",
    "length < 100 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8_max_games = MAX_GAMES_DEFAULT\n",
    "q8_n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "q8_rewards_self_n_stars = {n_star_q8: {'our_player': [], 'opponent':[]} for n_star_q8 in q8_n_stars}\n",
    "q8_M_opt_self_n_stars = {n_star_q8: [] for n_star_q8 in q8_n_stars}\n",
    "q8_M_rand_self_n_stars = {n_star_q8: [] for n_star_q8 in q8_n_stars}\n",
    "\n",
    "q8_min_epsilon = 0.1\n",
    "q8_max_epsilon = 0.8\n",
    "def q8_calc_epsilon_factory(n_star_q8, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q8_min_epsilon, q8_max_epsilon*(1-(real_game_number/n_star_q8)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q8 in q8_n_stars:\n",
    "    q8_q_learnt_player = QLearntPlayer(_epsilon=q8_max_epsilon)\n",
    "    print('Current n_star_q8 = {}'.format(n_star_q8))\n",
    "\n",
    "    for game_epoch in range(q8_max_games//epoch_size):\n",
    "        q8_calc_epsilon = q8_calc_epsilon_factory(n_star_q8=n_star_q8, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 8)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=epoch_size, _our_player=q8_q_learnt_player, \\\n",
    "            _our_player_new_game_epsilon=q8_calc_epsilon, _update_q_values=True)\n",
    "        q8_rewards_self_n_stars[n_star_q8]['our_player'] += run_rewards['our_player']\n",
    "        q8_rewards_self_n_stars[n_star_q8]['opponent'] += run_rewards['opponent']\n",
    "\n",
    "        # Run 500 games for q8_M_opt_self_n_stars calculation\n",
    "        q8_M_opt_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q8_q_learnt_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q8_M_opt_self_n_stars[n_star_q8].append(np.average(q8_M_opt_self_n_stars_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for q8_M_rand_self_n_stars calculation\n",
    "        q8_M_rand_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q8_q_learnt_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False)\n",
    "        q8_M_rand_self_n_stars[n_star_q8].append(np.average(q8_M_rand_self_n_stars_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = [250*i for i in range(MAX_GAMES_DEFAULT//250)]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, q8_M_rand_list) in enumerate(q8_M_rand_self_n_stars.items()):\n",
    "    plt.plot(episodes,q8_M_rand_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, q8_M_opt_list) in enumerate(q8_M_opt_self_n_stars.items()):\n",
    "    plt.plot(episodes, q8_M_opt_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q8\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "*Question 9. What are the highest values of $M_{opt}$ and $M_{rand}$ that you could achieve after playing 20’000 games?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decreasing epsilon\")\n",
    "for n_star in q8_M_rand_self_n_stars.keys():\n",
    "    print(f\"N star: {n_star} - Max M_rand : {max(q8_M_rand_self_n_stars[n_star])}\")\n",
    "\n",
    "print(\"Fixed epsilon\")\n",
    "for eps in q7_M_rand_self.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q7_M_rand_self[eps])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [np.array([[0.,0., 0.], \n",
    "                 [ 0., 0., 0.], \n",
    "                 [0.,0.,0.]]), \n",
    "        np.array([[1.,0., 0.], \n",
    "                 [ 0.,0., -1.], \n",
    "                 [0.,0.,1.]]),\n",
    "        np.array([[-1.,0.,0.], \n",
    "                 [ 1., 0., 0.], \n",
    "                 [ -1.,1.,0.]])]\n",
    "\n",
    "                 #player X (1) moves first if the number of pieces is equal, else it's O (-1)\n",
    "\n",
    "visualize_qvalues(q7_q_learnt_player, grids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep Q-Learning\n",
    "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch\n",
    "again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of\n",
    "Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is\n",
    "to approximate Q-values by a neural network instead of a look-up table as in Tabular Q-learning. For\n",
    "implementation, you can use ideas from the DQN tutorials of Keras and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Learning from experts\n",
    "Implement the DQN algorithm. To check the algorithm, run a DQN agent with a fixed and arbitrary\n",
    "$\\epsilon \\in [0,1)$ against Opt(0.5) for 20’000 games – switch the 1st player after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "*Plot average reward and average training loss for every 250 games during training. Does\n",
    "the loss decrease? Does the agent learn to play Tic Tac Toe?\n",
    "Expected answer: A figure with two subplots (caption length $<$ 50 words). Specify your choice of $\\epsilon$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We use $\\epsilon = 0.05$ as that is the value that Mnih et al (2015) use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q11_epsilon = 0.1 # Chosen because they use this in q2 so this will allow us to nicely compare\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "q11_max_games = MAX_GAMES_DEFAULT\n",
    "q11_dqn_player = DQN_Player(_epsilon=q11_epsilon)\n",
    "q11_epoch_size = 250\n",
    "q11_rewards = []\n",
    "q11_avgs = []\n",
    "q11_M_opt = []\n",
    "q11_M_rand = []\n",
    "q11_total_wins = 0\n",
    "q11_losses_per_epoch = []\n",
    "\n",
    "\n",
    "for game_epoch in range(q11_max_games//q11_epoch_size):\n",
    "    if game_epoch % 20 == 0:\n",
    "        print('Game ', game_epoch*q11_epoch_size, ' begins.')\n",
    "\n",
    "    # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "    q11_run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q11_dqn_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q11_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "    \n",
    "    q11_losses_per_epoch.append(q11_dqn_player.get_loss_and_reset_log())\n",
    "\n",
    "\n",
    "    q11_avgs.append(np.average(q11_run_rewards))\n",
    "    q11_rewards += q11_run_rewards\n",
    "    q11_total_wins += sum(1 if rew ==1 else 0 for rew in q11_run_rewards)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q11_M_opt_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q11_dqn_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q11_M_opt.append(np.average(q11_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q11_dqn_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q11_M_rand.append(np.average(M_rand_rewards))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4,nrows=1, figsize=(40,10))\n",
    "\n",
    "episodes = [ i*250 for i in range(len(q11_losses_per_epoch))]\n",
    "\n",
    "axs[0].plot(episodes,q11_losses_per_epoch)\n",
    "axs[0].set_title(\"Loss during training of the DQN agent\")\n",
    "axs[0].set_xlabel(\"Training game\")\n",
    "axs[0].set_ylabel(\"Loss average on 250 games\")\n",
    "\n",
    "\n",
    "axs[1].plot(episodes,q11_M_opt)\n",
    "axs[1].set_title(\"M opt score during training of the DQN agent\")\n",
    "axs[1].set_xlabel(\"Training game\")\n",
    "axs[1].set_ylabel(\"M opt\")\n",
    "\n",
    "axs[2].plot(episodes,q11_M_rand)\n",
    "axs[2].set_title(\"M rand score during training of the DQN agent\")\n",
    "axs[2].set_xlabel(\"Training game\")\n",
    "axs[2].set_ylabel(\"M rand\")\n",
    "\n",
    "\n",
    "axs[3].plot(episodes,q11_avgs)\n",
    "axs[3].set_title(\"Rewards during training of the DQN agent\")\n",
    "axs[3].set_xlabel(\"Training game\")\n",
    "axs[3].set_ylabel(\"Rewards\")\n",
    "\n",
    "plt.savefig(\"Q11\"+get_random_id()+\".svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q12_epsilon = 0.1 \n",
    "q12_max_games = MAX_GAMES_DEFAULT\n",
    "q12_dqn_player = DQN_Player(_epsilon=q12_epsilon, _batch_size=1, _replay_buffer_size=1)\n",
    "q12_epoch_size = 250\n",
    "q12_rewards = []\n",
    "q12_avgs = []\n",
    "q12_M_opt = []\n",
    "q12_M_rand = []\n",
    "q12_total_wins = 0\n",
    "\n",
    "q12_losses_per_epoch = []\n",
    "\n",
    "\n",
    "for game_epoch in range(q12_max_games//q12_epoch_size):\n",
    "    if game_epoch % 20 == 0:\n",
    "        print('Game ', game_epoch*q12_epoch_size, ' begins.')\n",
    "\n",
    "    # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "    q12_run_rewards = run_n_games_against_opt(_max_games_number=250, _our_player=q12_dqn_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: q12_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "    q12_avgs.append(np.average(q12_run_rewards))\n",
    "    q12_rewards += q12_run_rewards\n",
    "    q12_total_wins += sum(1 if rew ==1 else 0 for rew in q12_run_rewards)\n",
    "\n",
    "    q12_losses_per_epoch.append(q12_dqn_player.get_loss_and_reset_log())\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run 500 games for M_opt calculation\n",
    "        q12_M_opt_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q12_dqn_player, _opponent_epsilon=0, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q12_M_opt.append(np.average(q12_M_opt_rewards))\n",
    "\n",
    "\n",
    "        # Run 500 games for M_rand calculation\n",
    "        M_rand_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q12_dqn_player, _opponent_epsilon=1, \\\n",
    "            _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "        q12_M_rand.append(np.average(M_rand_rewards))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4,nrows=1, figsize=(40,10))\n",
    "\n",
    "episodes = [ i*250 for i in range(len(q12_losses_per_epoch))]\n",
    "\n",
    "axs[0].plot(episodes,q12_losses_per_epoch)\n",
    "axs[0].set_title(\"Loss during training of the DQN agent\")\n",
    "axs[0].set_xlabel(\"Training game\")\n",
    "axs[0].set_ylabel(\"Loss average on 250 games\")\n",
    "\n",
    "\n",
    "axs[1].plot(episodes,q12_M_opt)\n",
    "axs[1].set_title(\"M opt score during training of the DQN agent\")\n",
    "axs[1].set_xlabel(\"Training game\")\n",
    "axs[1].set_ylabel(\"M opt\")\n",
    "\n",
    "axs[2].plot(episodes,q12_M_rand)\n",
    "axs[2].set_title(\"M rand score during training of the DQN agent\")\n",
    "axs[2].set_xlabel(\"Training game\")\n",
    "axs[2].set_ylabel(\"M rand\")\n",
    "\n",
    "\n",
    "axs[3].plot(episodes,q12_avgs)\n",
    "axs[3].set_title(\"Rewards during training of the DQN agent\")\n",
    "axs[3].set_xlabel(\"Training game\")\n",
    "axs[3].set_ylabel(\"Rewards\")\n",
    "\n",
    "plt.savefig(\"Q12\"+get_random_id()+\".svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q13_calc_epsilon_factory(n_star_q13, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*q13_epoch_size + game_number_n\n",
    "            return max(q13_min_epsilon, q13_max_epsilon*(1-(real_game_number/n_star_q13)))\n",
    "        return _calc_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q13_max_games = MAX_GAMES_DEFAULT\n",
    "q13_n_stars = np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "q13_epoch_size = 250\n",
    "q13_rewards = {n_star_q13: [] for n_star_q13 in q13_n_stars}\n",
    "q13_avgs = {n_star_q13: [] for n_star_q13 in q13_n_stars}\n",
    "q13_M_opt = {n_star_q13: [] for n_star_q13 in q13_n_stars}\n",
    "q13_M_rand = {n_star_q13: [] for n_star_q13 in q13_n_stars}\n",
    "q13_total_wins = {n_star_q13: 0 for n_star_q13 in q13_n_stars}\n",
    "q13_losses = {n_star_q13: [] for n_star_q13 in q13_n_stars}\n",
    "\n",
    "\n",
    "q13_players = {}\n",
    "\n",
    "q13_min_epsilon = 0.1\n",
    "q13_max_epsilon = 0.8\n",
    "def q13_calc_epsilon_factory(n_star_q13, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*q13_epoch_size + game_number_n\n",
    "            return max(q13_min_epsilon, q13_max_epsilon*(1-(real_game_number/n_star_q13)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q13 in q13_n_stars:\n",
    "    q13_starting_epsilon  = q13_calc_epsilon_factory(n_star_q13, q13_epoch_size, 0)(0)\n",
    "    q13_dqn_player =  DQN_Player(_epsilon=q13_starting_epsilon, _batch_size=64, _replay_buffer_size=10_000) \n",
    "    q13_players[n_star_q13] = q13_dqn_player\n",
    "    print('Current n_star_q13 = {}'.format(n_star_q13))\n",
    "\n",
    "    for game_epoch in range(q13_max_games//q13_epoch_size):\n",
    "        q13_calc_epsilon = q13_calc_epsilon_factory(n_star_q13=n_star_q13, epoch_size=q13_epoch_size, game_epoch=game_epoch)\n",
    "\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q13_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 2)\n",
    "        q13_run_rewards = run_n_games_against_opt(_max_games_number=q13_epoch_size, _our_player=q13_dqn_player, _opponent_epsilon=0.5, \\\n",
    "            _our_player_new_game_epsilon=q13_calc_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "        q13_avgs[n_star_q13].append(np.average(q13_run_rewards))\n",
    "        q13_rewards[n_star_q13] += q13_run_rewards\n",
    "        q13_total_wins[n_star_q13] += sum(1 if rew ==1 else 0 for rew in q13_run_rewards)\n",
    "\n",
    "        q13_losses[n_star_q13].append(q13_dqn_player.get_loss_and_reset_log())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run 500 games for M_opt calculation\n",
    "            q13_M_opt_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q13_dqn_player, _opponent_epsilon=0, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q13_M_opt[n_star_q13].append(np.average(q13_M_opt_rewards))\n",
    "\n",
    "\n",
    "            # Run 500 games for M_rand calculation\n",
    "            M_rand_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q13_dqn_player, _opponent_epsilon=1, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q13_M_rand[n_star_q13].append(np.average(M_rand_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "episodes = [250*i for i in range(len(q13_M_rand[1.0]))]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, M_Rand_list) in enumerate(q13_M_rand.items()):\n",
    "    plt.plot(episodes,M_Rand_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, M_Opt_list) in enumerate(q13_M_opt.items()):\n",
    "    plt.plot(episodes, M_Opt_list,linestyle=\"-.\",color=colors[i], label = f\"{n_star}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q13\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q14_best_n_star = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q14_max_games = MAX_GAMES_DEFAULT\n",
    "q14_epoch_size = 250\n",
    "\n",
    "q14_eps_opts = np.linspace(0,1,num=5)\n",
    "\n",
    "q14_rewards_eps = {eps_opt: [] for eps_opt in q14_eps_opts}\n",
    "q14_M_opt_eps = {eps_opt: [] for eps_opt in q14_eps_opts}\n",
    "q14_M_rand_eps = {eps_opt: [] for eps_opt in q14_eps_opts}\n",
    "q14_losses = {eps_opt: [] for eps_opt in q14_eps_opts}\n",
    "\n",
    "q14_min_epsilon = 0.1\n",
    "q14_max_epsilon = 0.8\n",
    "\n",
    "\n",
    "def q14_calc_epsilon_factory_eps(epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q14_min_epsilon, q14_max_epsilon*(1-(real_game_number/q14_best_n_star)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for eps_opt in q14_eps_opts:\n",
    "    q14_starting_epsilon  = q14_calc_epsilon_factory_eps(q14_epoch_size, 0)(0)\n",
    "    q14_dqn_player =  DQN_Player(_epsilon=q14_starting_epsilon, _batch_size=64, _replay_buffer_size=10_000) \n",
    "\n",
    "    print('Current eps_opt = {}'.format(eps_opt))\n",
    "\n",
    "    for game_epoch in range(q14_max_games//q14_epoch_size):\n",
    "        q14_calc_epsilon = q14_calc_epsilon_factory_eps(epoch_size=q14_epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q14_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 4)\n",
    "        q14_run_rewards = run_n_games_against_opt(_max_games_number=q14_epoch_size, _our_player=q14_dqn_player, _opponent_epsilon=eps_opt, \\\n",
    "            _our_player_new_game_epsilon=q14_calc_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "        q14_rewards_eps[eps_opt] += q14_run_rewards\n",
    "\n",
    "        q14_losses[eps_opt].append(q14_dqn_player.get_loss_and_reset_log())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run 500 games for q14_M_opt_eps calculation\n",
    "            q14_M_opt_eps_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q14_dqn_player, _opponent_epsilon=0, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q14_M_opt_eps[eps_opt].append(np.average(q14_M_opt_eps_rewards))\n",
    "\n",
    "\n",
    "            # Run 500 games for q14_M_rand_eps calculation\n",
    "            q14_M_rand_eps_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q14_dqn_player, _opponent_epsilon=1, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q14_M_rand_eps[eps_opt].append(np.average(q14_M_rand_eps_rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "episodes = [250*i for i in range(len(q14_M_rand_eps[1.0]))]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, M_Rand_list) in enumerate(q14_M_rand_eps.items()):\n",
    "    plt.plot(episodes,M_Rand_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, M_Opt_list) in enumerate(q14_M_opt_eps.items()):\n",
    "    plt.plot(episodes, M_Opt_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q14\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in q14_M_rand_eps.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q14_M_rand_eps[eps])}\")\n",
    "\n",
    "for n_star in q13_M_rand.keys():\n",
    "    print(f\"N star: {n_star} - Max M_rand : {max(q13_M_rand[n_star])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in q14_M_opt_eps.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q14_M_opt_eps[eps])}\")\n",
    "\n",
    "for n_star in q13_M_opt.keys():\n",
    "    print(f\"N star: {n_star} - Max M_rand : {max(q13_M_opt[n_star])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q16_max_games = MAX_GAMES_DEFAULT\n",
    "q16_epoch_size = 250\n",
    "\n",
    "q16_eps_selfs =  np.linspace(0,0.99,num=5)\n",
    "\n",
    "q16_M_opt_self = {eps_opt: [] for eps_opt in q16_eps_selfs}\n",
    "q16_M_rand_self = {eps_opt: [] for eps_opt in q16_eps_selfs}\n",
    "\n",
    "q16_min_epsilon = 0.1\n",
    "q16_max_epsilon = 0.8\n",
    "\n",
    "n_star = None\n",
    "calc_epsilon = None\n",
    "\n",
    "for eps_s in q16_eps_selfs:\n",
    "    q16_dqn_player =  DQN_Player(_epsilon=eps_s, _batch_size=64, _replay_buffer_size=10_000) \n",
    "    print('Current eps_s = {}'.format(eps_s))\n",
    "\n",
    "    for game_epoch in range(q16_max_games//q16_epoch_size):\n",
    "        q16_calc_epsilon = lambda game_number_n: eps_s\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*q16_epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 7)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=q16_epoch_size, _our_player=q16_dqn_player, \\\n",
    "            _our_player_new_game_epsilon=q16_calc_epsilon, _update_q_values=True,_throw_error_on_illegal_moves=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run 500 games for q16_M_opt_self calculation\n",
    "            q16_M_opt_self_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q16_dqn_player, _opponent_epsilon=0, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False,_throw_error_on_illegal_moves=False)\n",
    "            q16_M_opt_self[eps_s].append(np.average(q16_M_opt_self_rewards))\n",
    "\n",
    "            \n",
    "            # Run 500 games for q16_M_rand_self calculation\n",
    "            q16_M_rand_self_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q16_dqn_player, _opponent_epsilon=1, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False,_throw_error_on_illegal_moves=False)\n",
    "            q16_M_rand_self[eps_s].append(np.average(q16_M_rand_self_rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "episodes = [250*i for i in range(len(q16_M_rand_self[0.]))]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(eps, M_Rand_list) in enumerate(q16_M_rand_self.items()):\n",
    "    plt.plot(episodes,M_Rand_list,linestyle=\"solid\", color=colors[i], label = f\"{eps}\")\n",
    "\n",
    "for i,(eps, M_Opt_list) in enumerate(q16_M_opt_self.items()):\n",
    "    plt.plot(episodes, M_Opt_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q16\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q17_max_games = MAX_GAMES_DEFAULT\n",
    "q17_n_stars =  np.geomspace(1, 40000, num=5) # Includes 1 and 40000\n",
    "epoch_size = 250\n",
    "\n",
    "q17_rewards_self_n_stars = {n_star_q17: {'our_player': [], 'opponent':[]} for n_star_q17 in q17_n_stars}\n",
    "q17_M_opt_self_n_stars = {n_star_q17: [] for n_star_q17 in q17_n_stars}\n",
    "q17_M_rand_self_n_stars = {n_star_q17: [] for n_star_q17 in q17_n_stars}\n",
    "\n",
    "q17_min_epsilon = 0.1\n",
    "q17_max_epsilon = 0.8\n",
    "\n",
    "def q17_calc_epsilon_factory(n_star_q17, epoch_size, game_epoch):\n",
    "        def _calc_epsilon(game_number_n):\n",
    "            real_game_number = game_epoch*epoch_size + game_number_n\n",
    "            return max(q17_min_epsilon, q17_max_epsilon*(1-(real_game_number/n_star_q17)))\n",
    "        return _calc_epsilon\n",
    "\n",
    "for n_star_q17 in q17_n_stars:\n",
    "    q17_dqn_player = DQN_Player(_epsilon=q17_max_epsilon, _batch_size=64, _replay_buffer_size=10_000) \n",
    "    print('Current n_star_q17 = {}'.format(n_star_q17))\n",
    "\n",
    "    for game_epoch in range(q17_max_games//epoch_size):\n",
    "        q17_calc_epsilon = q17_calc_epsilon_factory(n_star_q17=n_star_q17, epoch_size=epoch_size, game_epoch=game_epoch)\n",
    "        if game_epoch % 20 == 0:\n",
    "            print('Game ', game_epoch*epoch_size, ' begins.')\n",
    "\n",
    "        # Run 250 games with updating Q-vals and observe reward (exec 8)\n",
    "        run_rewards = run_n_games_against_self(_max_games_number=epoch_size, _our_player=q17_dqn_player, \\\n",
    "            _our_player_new_game_epsilon=q17_calc_epsilon, _update_q_values=True, _throw_error_on_illegal_moves=False)\n",
    "        q17_rewards_self_n_stars[n_star_q17]['our_player'] += run_rewards['our_player']\n",
    "        q17_rewards_self_n_stars[n_star_q17]['opponent'] += run_rewards['opponent']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run 500 games for q17_M_opt_self_n_stars calculation\n",
    "            q17_M_opt_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q17_dqn_player, _opponent_epsilon=0, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q17_M_opt_self_n_stars[n_star_q17].append(np.average(q17_M_opt_self_n_stars_rewards))\n",
    "\n",
    "\n",
    "            # Run 500 games for q17_M_rand_self_n_stars calculation\n",
    "            q17_M_rand_self_n_stars_rewards = run_n_games_against_opt(_max_games_number=GAMES_FOR_EVAL, _our_player=q17_dqn_player, _opponent_epsilon=1, \\\n",
    "                _our_player_new_game_epsilon=lambda game_number_n: 0, _update_q_values=False, _throw_error_on_illegal_moves=False)\n",
    "            q17_M_rand_self_n_stars[n_star_q17].append(np.average(q17_M_rand_self_n_stars_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "episodes = [250*i for i in range(len(q17_M_rand_self_n_stars[1.0]))]\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "colors = [\"C\"+str(j) for j in range(5)]\n",
    "\n",
    "for i,(n_star, M_Rand_list) in enumerate(q17_M_rand_self_n_stars.items()):\n",
    "    plt.plot(episodes,M_Rand_list,linestyle=\"solid\", color=colors[i], label = f\"{n_star}\")\n",
    "\n",
    "for i,(n_star, M_Opt_list) in enumerate(q17_M_opt_self_n_stars.items()):\n",
    "    plt.plot(episodes, M_Opt_list,linestyle=\"-.\",color=colors[i])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('M')\n",
    "plt.xlabel('Training game')\n",
    "plt.savefig(\"Q17\"+get_random_id()+\".svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in q16_M_rand_self.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q16_M_rand_self[eps])}\")\n",
    "\n",
    "for n_star in q17_M_rand_self_n_stars.keys():\n",
    "    print(f\"N star: {n_star} - Max M_rand : {max(q17_M_rand_self_n_stars[n_star])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in q16_M_opt_self.keys():\n",
    "    print(f\"Epsilon: {eps} - Max M_rand : {max(q16_M_opt_self[eps])}\")\n",
    "\n",
    "for n_star in q17_M_opt_self_n_stars.keys():\n",
    "    print(f\"N star: {n_star} - Max M_rand : {max(q17_M_opt_self_n_stars[n_star])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [np.array([[0.,0., 0.], \n",
    "                 [ 0., 0., 0.], \n",
    "                 [0.,0.,0.]]), \n",
    "        np.array([[1.,0., 0.], \n",
    "                 [ 0.,0., -1.], \n",
    "                 [0.,0.,1.]]),\n",
    "        np.array([[1.,0.,0.], \n",
    "                 [ -1., 0., 0.], \n",
    "                 [ 1.,-1.,0.]])]\n",
    "\n",
    "\n",
    "visualize_qvalues(q17_dqn_player, grids)\n",
    "plt.savefig(\"Q19\"+get_random_id()+\".svg\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50c8ddb039f6f0c33e85a1fe59b867fa411b457638d3d4f1beacaa03741717f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mausspaun')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
